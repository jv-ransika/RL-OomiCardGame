{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96908cc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T04:50:48.769423Z",
     "iopub.status.busy": "2024-07-14T04:50:48.768953Z",
     "iopub.status.idle": "2024-07-14T10:44:46.277711Z",
     "shell.execute_reply": "2024-07-14T10:44:46.276726Z"
    },
    "papermill": {
     "duration": 21237.600141,
     "end_time": "2024-07-14T10:44:46.362613",
     "exception": false,
     "start_time": "2024-07-14T04:50:48.762472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading existing model...\n",
      "Continuing training for 100000 more episodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 completed. Mean loss: nan, Mean reward: 0.0125\n",
      "Episode 100 completed. Mean loss: -0.2128, Mean reward: 0.0160\n",
      "Episode 200 completed. Mean loss: -0.2620, Mean reward: 0.0115\n",
      "Episode 300 completed. Mean loss: -0.0534, Mean reward: 0.0050\n",
      "Episode 400 completed. Mean loss: -0.0107, Mean reward: 0.0075\n",
      "Episode 500 completed. Mean loss: -0.2058, Mean reward: 0.0015\n",
      "Episode 600 completed. Mean loss: -0.0176, Mean reward: 0.0095\n",
      "Episode 700 completed. Mean loss: -0.2418, Mean reward: 0.0015\n",
      "Episode 800 completed. Mean loss: -0.1349, Mean reward: 0.0125\n",
      "Episode 900 completed. Mean loss: 0.0492, Mean reward: 0.0150\n",
      "Episode 1000 completed. Mean loss: -0.0228, Mean reward: 0.0035\n",
      "Episode 1100 completed. Mean loss: -0.2138, Mean reward: 0.0180\n",
      "Episode 1200 completed. Mean loss: -0.2668, Mean reward: 0.0035\n",
      "Episode 1300 completed. Mean loss: -0.1216, Mean reward: 0.0075\n",
      "Episode 1400 completed. Mean loss: -0.3955, Mean reward: 0.0160\n",
      "Episode 1500 completed. Mean loss: 0.0503, Mean reward: 0.0155\n",
      "Episode 1600 completed. Mean loss: -0.0727, Mean reward: 0.0075\n",
      "Episode 1700 completed. Mean loss: -0.2797, Mean reward: 0.0170\n",
      "Episode 1800 completed. Mean loss: -0.4303, Mean reward: 0.0175\n",
      "Episode 1900 completed. Mean loss: -0.3154, Mean reward: 0.0050\n",
      "Episode 2000 completed. Mean loss: 0.0019, Mean reward: 0.0180\n",
      "Episode 2100 completed. Mean loss: -0.3678, Mean reward: 0.0170\n",
      "Episode 2200 completed. Mean loss: -0.4347, Mean reward: 0.0055\n",
      "Episode 2300 completed. Mean loss: -0.3743, Mean reward: 0.0090\n",
      "Episode 2400 completed. Mean loss: -0.2313, Mean reward: 0.0135\n",
      "Episode 2500 completed. Mean loss: -0.2096, Mean reward: 0.0130\n",
      "Episode 2600 completed. Mean loss: -0.4588, Mean reward: 0.0130\n",
      "Episode 2700 completed. Mean loss: -0.3375, Mean reward: 0.0115\n",
      "Episode 2800 completed. Mean loss: -0.2345, Mean reward: 0.0115\n",
      "Episode 2900 completed. Mean loss: -0.0610, Mean reward: 0.0010\n",
      "Episode 3000 completed. Mean loss: -0.2659, Mean reward: 0.0135\n",
      "Episode 3100 completed. Mean loss: -0.1397, Mean reward: 0.0110\n",
      "Episode 3200 completed. Mean loss: -0.5556, Mean reward: 0.0115\n",
      "Episode 3300 completed. Mean loss: -0.2550, Mean reward: 0.0150\n",
      "Episode 3400 completed. Mean loss: -0.5043, Mean reward: 0.0075\n",
      "Episode 3500 completed. Mean loss: -0.3214, Mean reward: 0.0095\n",
      "Episode 3600 completed. Mean loss: -0.1390, Mean reward: 0.0080\n",
      "Episode 3700 completed. Mean loss: -0.0169, Mean reward: 0.0070\n",
      "Episode 3800 completed. Mean loss: 0.1226, Mean reward: 0.0075\n",
      "Episode 3900 completed. Mean loss: -0.0735, Mean reward: 0.0095\n",
      "Episode 4000 completed. Mean loss: -0.3282, Mean reward: 0.0055\n",
      "Episode 4100 completed. Mean loss: -0.1377, Mean reward: 0.0075\n",
      "Episode 4200 completed. Mean loss: -0.0129, Mean reward: 0.0090\n",
      "Episode 4300 completed. Mean loss: -0.2417, Mean reward: 0.0035\n",
      "Episode 4400 completed. Mean loss: -0.2095, Mean reward: 0.0030\n",
      "Episode 4500 completed. Mean loss: -0.0291, Mean reward: 0.0135\n",
      "Episode 4600 completed. Mean loss: -0.1477, Mean reward: 0.0195\n",
      "Episode 4700 completed. Mean loss: 0.0133, Mean reward: 0.0010\n",
      "Episode 4800 completed. Mean loss: -0.3629, Mean reward: 0.0095\n",
      "Episode 4900 completed. Mean loss: -0.0543, Mean reward: 0.0090\n",
      "Episode 5000 completed. Mean loss: -0.1597, Mean reward: 0.0060\n",
      "Episode 5100 completed. Mean loss: -0.2858, Mean reward: 0.0195\n",
      "Episode 5200 completed. Mean loss: -0.3856, Mean reward: -0.0005\n",
      "Episode 5300 completed. Mean loss: 0.0002, Mean reward: 0.0120\n",
      "Episode 5400 completed. Mean loss: -0.1533, Mean reward: 0.0075\n",
      "Episode 5500 completed. Mean loss: -0.3959, Mean reward: 0.0030\n",
      "Episode 5600 completed. Mean loss: -0.3606, Mean reward: 0.0120\n",
      "Episode 5700 completed. Mean loss: -0.3324, Mean reward: 0.0060\n",
      "Episode 5800 completed. Mean loss: -0.3450, Mean reward: 0.0050\n",
      "Episode 5900 completed. Mean loss: -0.6434, Mean reward: 0.0050\n",
      "Episode 6000 completed. Mean loss: -0.3496, Mean reward: 0.0060\n",
      "Episode 6100 completed. Mean loss: -0.3369, Mean reward: 0.0180\n",
      "Episode 6200 completed. Mean loss: -0.4853, Mean reward: 0.0205\n",
      "Episode 6300 completed. Mean loss: -0.4828, Mean reward: 0.0060\n",
      "Episode 6400 completed. Mean loss: -0.3088, Mean reward: 0.0055\n",
      "Episode 6500 completed. Mean loss: -0.2273, Mean reward: 0.0170\n",
      "Episode 6600 completed. Mean loss: -0.4170, Mean reward: 0.0080\n",
      "Episode 6700 completed. Mean loss: -0.4656, Mean reward: 0.0035\n",
      "Episode 6800 completed. Mean loss: -0.7318, Mean reward: 0.0095\n",
      "Episode 6900 completed. Mean loss: -0.3884, Mean reward: 0.0140\n",
      "Episode 7000 completed. Mean loss: -0.4671, Mean reward: 0.0055\n",
      "Episode 7100 completed. Mean loss: -0.4722, Mean reward: 0.0100\n",
      "Episode 7200 completed. Mean loss: -0.3462, Mean reward: 0.0070\n",
      "Episode 7300 completed. Mean loss: -0.3469, Mean reward: 0.0020\n",
      "Episode 7400 completed. Mean loss: -0.2070, Mean reward: 0.0060\n",
      "Episode 7500 completed. Mean loss: -0.3815, Mean reward: 0.0095\n",
      "Episode 7600 completed. Mean loss: -0.2639, Mean reward: 0.0115\n",
      "Episode 7700 completed. Mean loss: -0.3210, Mean reward: 0.0170\n",
      "Episode 7800 completed. Mean loss: -0.6313, Mean reward: 0.0160\n",
      "Episode 7900 completed. Mean loss: -0.2507, Mean reward: 0.0115\n",
      "Episode 8000 completed. Mean loss: -0.4422, Mean reward: 0.0115\n",
      "Episode 8100 completed. Mean loss: -0.5100, Mean reward: 0.0175\n",
      "Episode 8200 completed. Mean loss: -0.4327, Mean reward: 0.0030\n",
      "Episode 8300 completed. Mean loss: -0.2217, Mean reward: 0.0255\n",
      "Episode 8400 completed. Mean loss: -0.2981, Mean reward: 0.0060\n",
      "Episode 8500 completed. Mean loss: -0.5482, Mean reward: 0.0075\n",
      "Episode 8600 completed. Mean loss: -0.4112, Mean reward: 0.0160\n",
      "Episode 8700 completed. Mean loss: -0.5067, Mean reward: 0.0090\n",
      "Episode 8800 completed. Mean loss: -0.5501, Mean reward: 0.0115\n",
      "Episode 8900 completed. Mean loss: -0.3180, Mean reward: 0.0035\n",
      "Episode 9000 completed. Mean loss: -0.3248, Mean reward: 0.0080\n",
      "Episode 9100 completed. Mean loss: -0.3105, Mean reward: 0.0110\n",
      "Episode 9200 completed. Mean loss: -0.2538, Mean reward: 0.0075\n",
      "Episode 9300 completed. Mean loss: -0.4826, Mean reward: 0.0015\n",
      "Episode 9400 completed. Mean loss: -0.2973, Mean reward: 0.0110\n",
      "Episode 9500 completed. Mean loss: -0.3837, Mean reward: 0.0015\n",
      "Episode 9600 completed. Mean loss: -0.3394, Mean reward: 0.0150\n",
      "Episode 9700 completed. Mean loss: -0.3182, Mean reward: 0.0080\n",
      "Episode 9800 completed. Mean loss: -0.2929, Mean reward: 0.0095\n",
      "Episode 9900 completed. Mean loss: -0.3456, Mean reward: 0.0050\n",
      "Episode 10000 completed. Mean loss: -0.4505, Mean reward: 0.0155\n",
      "Episode 10100 completed. Mean loss: -0.3898, Mean reward: 0.0140\n",
      "Episode 10200 completed. Mean loss: -0.3404, Mean reward: 0.0150\n",
      "Episode 10300 completed. Mean loss: -0.1362, Mean reward: 0.0070\n",
      "Episode 10400 completed. Mean loss: -0.2968, Mean reward: -0.0000\n",
      "Episode 10500 completed. Mean loss: -0.5149, Mean reward: 0.0150\n",
      "Episode 10600 completed. Mean loss: -0.1406, Mean reward: 0.0055\n",
      "Episode 10700 completed. Mean loss: -0.4373, Mean reward: 0.0110\n",
      "Episode 10800 completed. Mean loss: -0.2417, Mean reward: -0.0005\n",
      "Episode 10900 completed. Mean loss: -0.4901, Mean reward: 0.0040\n",
      "Episode 11000 completed. Mean loss: -0.5322, Mean reward: 0.0095\n",
      "Episode 11100 completed. Mean loss: -0.5035, Mean reward: 0.0080\n",
      "Episode 11200 completed. Mean loss: -0.2624, Mean reward: 0.0180\n",
      "Episode 11300 completed. Mean loss: -0.3537, Mean reward: 0.0070\n",
      "Episode 11400 completed. Mean loss: -0.1376, Mean reward: 0.0160\n",
      "Episode 11500 completed. Mean loss: -0.1821, Mean reward: 0.0100\n",
      "Episode 11600 completed. Mean loss: -0.4534, Mean reward: 0.0080\n",
      "Episode 11700 completed. Mean loss: -0.3989, Mean reward: 0.0075\n",
      "Episode 11800 completed. Mean loss: -0.3931, Mean reward: 0.0155\n",
      "Episode 11900 completed. Mean loss: -0.4797, Mean reward: 0.0100\n",
      "Episode 12000 completed. Mean loss: -0.4066, Mean reward: 0.0140\n",
      "Episode 12100 completed. Mean loss: -0.4047, Mean reward: 0.0130\n",
      "Episode 12200 completed. Mean loss: -0.8983, Mean reward: 0.0075\n",
      "Episode 12300 completed. Mean loss: -0.3935, Mean reward: 0.0235\n",
      "Episode 12400 completed. Mean loss: -0.5980, Mean reward: 0.0010\n",
      "Episode 12500 completed. Mean loss: -0.3756, Mean reward: 0.0110\n",
      "Episode 12600 completed. Mean loss: -0.2675, Mean reward: 0.0100\n",
      "Episode 12700 completed. Mean loss: -0.0527, Mean reward: 0.0130\n",
      "Episode 12800 completed. Mean loss: -0.2854, Mean reward: 0.0030\n",
      "Episode 12900 completed. Mean loss: -0.3489, Mean reward: 0.0095\n",
      "Episode 13000 completed. Mean loss: -0.6199, Mean reward: 0.0135\n",
      "Episode 13100 completed. Mean loss: 0.0612, Mean reward: 0.0070\n",
      "Episode 13200 completed. Mean loss: 0.0007, Mean reward: 0.0090\n",
      "Episode 13300 completed. Mean loss: 0.0262, Mean reward: 0.0070\n",
      "Episode 13400 completed. Mean loss: -0.2113, Mean reward: 0.0120\n",
      "Episode 13500 completed. Mean loss: 0.0588, Mean reward: 0.0135\n",
      "Episode 13600 completed. Mean loss: -0.2457, Mean reward: 0.0140\n",
      "Episode 13700 completed. Mean loss: 0.0205, Mean reward: 0.0135\n",
      "Episode 13800 completed. Mean loss: -0.0744, Mean reward: -0.0000\n",
      "Episode 13900 completed. Mean loss: -0.0758, Mean reward: 0.0180\n",
      "Episode 14000 completed. Mean loss: -0.1058, Mean reward: 0.0035\n",
      "Episode 14100 completed. Mean loss: -0.4621, Mean reward: 0.0070\n",
      "Episode 14200 completed. Mean loss: -0.2017, Mean reward: 0.0170\n",
      "Episode 14300 completed. Mean loss: -0.1504, Mean reward: 0.0130\n",
      "Episode 14400 completed. Mean loss: -0.2899, Mean reward: 0.0090\n",
      "Episode 14500 completed. Mean loss: -0.2377, Mean reward: 0.0075\n",
      "Episode 14600 completed. Mean loss: -0.0953, Mean reward: 0.0075\n",
      "Episode 14700 completed. Mean loss: -0.1479, Mean reward: 0.0015\n",
      "Episode 14800 completed. Mean loss: -0.1620, Mean reward: 0.0135\n",
      "Episode 14900 completed. Mean loss: -0.1485, Mean reward: 0.0010\n",
      "Episode 15000 completed. Mean loss: -0.1165, Mean reward: 0.0090\n",
      "Episode 15100 completed. Mean loss: -0.2140, Mean reward: 0.0110\n",
      "Episode 15200 completed. Mean loss: 0.0760, Mean reward: 0.0090\n",
      "Episode 15300 completed. Mean loss: -0.0933, Mean reward: 0.0150\n",
      "Episode 15400 completed. Mean loss: -0.1264, Mean reward: 0.0090\n",
      "Episode 15500 completed. Mean loss: 0.0081, Mean reward: 0.0155\n",
      "Episode 15600 completed. Mean loss: 0.0523, Mean reward: 0.0115\n",
      "Episode 15700 completed. Mean loss: -0.1632, Mean reward: 0.0115\n",
      "Episode 15800 completed. Mean loss: -0.1078, Mean reward: 0.0150\n",
      "Episode 15900 completed. Mean loss: -0.0981, Mean reward: 0.0215\n",
      "Episode 16000 completed. Mean loss: 0.0437, Mean reward: 0.0190\n",
      "Episode 16100 completed. Mean loss: -0.0879, Mean reward: -0.0010\n",
      "Episode 16200 completed. Mean loss: -0.2964, Mean reward: 0.0170\n",
      "Episode 16300 completed. Mean loss: -0.0063, Mean reward: 0.0090\n",
      "Episode 16400 completed. Mean loss: 0.0777, Mean reward: 0.0085\n",
      "Episode 16500 completed. Mean loss: -0.1371, Mean reward: 0.0095\n",
      "Episode 16600 completed. Mean loss: -0.1215, Mean reward: 0.0210\n",
      "Episode 16700 completed. Mean loss: -0.0088, Mean reward: 0.0135\n",
      "Episode 16800 completed. Mean loss: -0.2328, Mean reward: 0.0010\n",
      "Episode 16900 completed. Mean loss: -0.0367, Mean reward: -0.0005\n",
      "Episode 17000 completed. Mean loss: -0.1961, Mean reward: 0.0060\n",
      "Episode 17100 completed. Mean loss: -0.0194, Mean reward: 0.0110\n",
      "Episode 17200 completed. Mean loss: -0.2982, Mean reward: 0.0050\n",
      "Episode 17300 completed. Mean loss: -0.2449, Mean reward: 0.0110\n",
      "Episode 17400 completed. Mean loss: -0.3299, Mean reward: 0.0115\n",
      "Episode 17500 completed. Mean loss: -0.0395, Mean reward: 0.0155\n",
      "Episode 17600 completed. Mean loss: -0.2474, Mean reward: 0.0070\n",
      "Episode 17700 completed. Mean loss: -0.2619, Mean reward: 0.0070\n",
      "Episode 17800 completed. Mean loss: -0.3862, Mean reward: 0.0090\n",
      "Episode 17900 completed. Mean loss: -0.1974, Mean reward: 0.0135\n",
      "Episode 18000 completed. Mean loss: -0.6193, Mean reward: 0.0155\n",
      "Episode 18100 completed. Mean loss: -0.1519, Mean reward: 0.0020\n",
      "Episode 18200 completed. Mean loss: -0.2301, Mean reward: 0.0090\n",
      "Episode 18300 completed. Mean loss: -0.3770, Mean reward: 0.0090\n",
      "Episode 18400 completed. Mean loss: -0.2379, Mean reward: 0.0110\n",
      "Episode 18500 completed. Mean loss: -0.2749, Mean reward: 0.0160\n",
      "Episode 18600 completed. Mean loss: -0.1884, Mean reward: 0.0035\n",
      "Episode 18700 completed. Mean loss: -0.4131, Mean reward: 0.0015\n",
      "Episode 18800 completed. Mean loss: -0.2204, Mean reward: 0.0115\n",
      "Episode 18900 completed. Mean loss: -0.4025, Mean reward: 0.0090\n",
      "Episode 19000 completed. Mean loss: -0.3879, Mean reward: 0.0140\n",
      "Episode 19100 completed. Mean loss: -0.4295, Mean reward: 0.0135\n",
      "Episode 19200 completed. Mean loss: -0.1255, Mean reward: 0.0100\n",
      "Episode 19300 completed. Mean loss: -0.1071, Mean reward: 0.0210\n",
      "Episode 19400 completed. Mean loss: -0.3958, Mean reward: 0.0200\n",
      "Episode 19500 completed. Mean loss: -0.4197, Mean reward: 0.0110\n",
      "Episode 19600 completed. Mean loss: -0.1954, Mean reward: 0.0130\n",
      "Episode 19700 completed. Mean loss: -0.2314, Mean reward: 0.0120\n",
      "Episode 19800 completed. Mean loss: -0.2410, Mean reward: 0.0135\n",
      "Episode 19900 completed. Mean loss: -0.2836, Mean reward: 0.0055\n",
      "Episode 20000 completed. Mean loss: -0.0883, Mean reward: 0.0095\n",
      "Episode 20100 completed. Mean loss: -0.3043, Mean reward: 0.0160\n",
      "Episode 20200 completed. Mean loss: -0.1652, Mean reward: 0.0135\n",
      "Episode 20300 completed. Mean loss: -0.4717, Mean reward: 0.0075\n",
      "Episode 20400 completed. Mean loss: -0.2293, Mean reward: 0.0050\n",
      "Episode 20500 completed. Mean loss: -0.4085, Mean reward: 0.0015\n",
      "Episode 20600 completed. Mean loss: -0.2603, Mean reward: 0.0055\n",
      "Episode 20700 completed. Mean loss: 0.1128, Mean reward: 0.0170\n",
      "Episode 20800 completed. Mean loss: -0.3763, Mean reward: 0.0140\n",
      "Episode 20900 completed. Mean loss: -0.1773, Mean reward: 0.0075\n",
      "Episode 21000 completed. Mean loss: -0.1665, Mean reward: 0.0175\n",
      "Episode 21100 completed. Mean loss: -0.5052, Mean reward: 0.0050\n",
      "Episode 21200 completed. Mean loss: -0.2031, Mean reward: 0.0130\n",
      "Episode 21300 completed. Mean loss: -0.4308, Mean reward: 0.0115\n",
      "Episode 21400 completed. Mean loss: -0.2340, Mean reward: 0.0080\n",
      "Episode 21500 completed. Mean loss: -0.1275, Mean reward: 0.0115\n",
      "Episode 21600 completed. Mean loss: -0.4913, Mean reward: 0.0115\n",
      "Episode 21700 completed. Mean loss: -0.4198, Mean reward: 0.0095\n",
      "Episode 21800 completed. Mean loss: -0.0373, Mean reward: 0.0150\n",
      "Episode 21900 completed. Mean loss: 0.0839, Mean reward: 0.0110\n",
      "Episode 22000 completed. Mean loss: -0.3202, Mean reward: 0.0195\n",
      "Episode 22100 completed. Mean loss: -0.2078, Mean reward: 0.0070\n",
      "Episode 22200 completed. Mean loss: -0.1383, Mean reward: 0.0190\n",
      "Episode 22300 completed. Mean loss: -0.0960, Mean reward: 0.0170\n",
      "Episode 22400 completed. Mean loss: -0.0142, Mean reward: 0.0100\n",
      "Episode 22500 completed. Mean loss: -0.2810, Mean reward: 0.0190\n",
      "Episode 22600 completed. Mean loss: -0.1899, Mean reward: 0.0110\n",
      "Episode 22700 completed. Mean loss: -0.2956, Mean reward: 0.0065\n",
      "Episode 22800 completed. Mean loss: -0.0880, Mean reward: 0.0095\n",
      "Episode 22900 completed. Mean loss: 0.0106, Mean reward: 0.0030\n",
      "Episode 23000 completed. Mean loss: -0.0867, Mean reward: 0.0150\n",
      "Episode 23100 completed. Mean loss: -0.3877, Mean reward: 0.0090\n",
      "Episode 23200 completed. Mean loss: -0.1577, Mean reward: 0.0070\n",
      "Episode 23300 completed. Mean loss: 0.0481, Mean reward: 0.0190\n",
      "Episode 23400 completed. Mean loss: -0.1999, Mean reward: -0.0010\n",
      "Episode 23500 completed. Mean loss: -0.3940, Mean reward: 0.0095\n",
      "Episode 23600 completed. Mean loss: 0.0703, Mean reward: 0.0095\n",
      "Episode 23700 completed. Mean loss: -0.1425, Mean reward: 0.0110\n",
      "Episode 23800 completed. Mean loss: -0.3425, Mean reward: 0.0055\n",
      "Episode 23900 completed. Mean loss: -0.3122, Mean reward: 0.0190\n",
      "Episode 24000 completed. Mean loss: -0.2059, Mean reward: 0.0030\n",
      "Episode 24100 completed. Mean loss: -0.2834, Mean reward: 0.0155\n",
      "Episode 24200 completed. Mean loss: -0.1896, Mean reward: 0.0120\n",
      "Episode 24300 completed. Mean loss: -0.2542, Mean reward: 0.0050\n",
      "Episode 24400 completed. Mean loss: -0.3941, Mean reward: 0.0025\n",
      "Episode 24500 completed. Mean loss: -0.3329, Mean reward: 0.0160\n",
      "Episode 24600 completed. Mean loss: -0.2976, Mean reward: 0.0050\n",
      "Episode 24700 completed. Mean loss: 0.0560, Mean reward: 0.0155\n",
      "Episode 24800 completed. Mean loss: -0.3771, Mean reward: -0.0010\n",
      "Episode 24900 completed. Mean loss: -0.0974, Mean reward: 0.0035\n",
      "Episode 25000 completed. Mean loss: -0.1242, Mean reward: 0.0110\n",
      "Episode 25100 completed. Mean loss: -0.3358, Mean reward: 0.0110\n",
      "Episode 25200 completed. Mean loss: -0.1503, Mean reward: 0.0175\n",
      "Episode 25300 completed. Mean loss: -0.3337, Mean reward: 0.0150\n",
      "Episode 25400 completed. Mean loss: -0.0763, Mean reward: 0.0090\n",
      "Episode 25500 completed. Mean loss: -0.0500, Mean reward: 0.0155\n",
      "Episode 25600 completed. Mean loss: -0.0700, Mean reward: 0.0095\n",
      "Episode 25700 completed. Mean loss: -0.2342, Mean reward: 0.0065\n",
      "Episode 25800 completed. Mean loss: -0.0566, Mean reward: 0.0050\n",
      "Episode 25900 completed. Mean loss: -0.1273, Mean reward: 0.0235\n",
      "Episode 26000 completed. Mean loss: 0.0580, Mean reward: 0.0080\n",
      "Episode 26100 completed. Mean loss: -0.1421, Mean reward: 0.0095\n",
      "Episode 26200 completed. Mean loss: -0.3067, Mean reward: 0.0060\n",
      "Episode 26300 completed. Mean loss: -0.3183, Mean reward: 0.0150\n",
      "Episode 26400 completed. Mean loss: -0.2919, Mean reward: 0.0090\n",
      "Episode 26500 completed. Mean loss: -0.3410, Mean reward: 0.0180\n",
      "Episode 26600 completed. Mean loss: -0.2440, Mean reward: 0.0195\n",
      "Episode 26700 completed. Mean loss: -0.1193, Mean reward: 0.0090\n",
      "Episode 26800 completed. Mean loss: -0.2490, Mean reward: 0.0015\n",
      "Episode 26900 completed. Mean loss: -0.3000, Mean reward: 0.0050\n",
      "Episode 27000 completed. Mean loss: -0.3767, Mean reward: 0.0070\n",
      "Episode 27100 completed. Mean loss: -0.2744, Mean reward: 0.0030\n",
      "Episode 27200 completed. Mean loss: -0.3891, Mean reward: 0.0115\n",
      "Episode 27300 completed. Mean loss: -0.5097, Mean reward: 0.0010\n",
      "Episode 27400 completed. Mean loss: -0.2378, Mean reward: 0.0150\n",
      "Episode 27500 completed. Mean loss: -0.0831, Mean reward: 0.0070\n",
      "Episode 27600 completed. Mean loss: -0.3893, Mean reward: 0.0155\n",
      "Episode 27700 completed. Mean loss: -0.3868, Mean reward: 0.0115\n",
      "Episode 27800 completed. Mean loss: -0.2213, Mean reward: 0.0190\n",
      "Episode 27900 completed. Mean loss: -0.1603, Mean reward: 0.0140\n",
      "Episode 28000 completed. Mean loss: -0.3637, Mean reward: 0.0055\n",
      "Episode 28100 completed. Mean loss: -0.3999, Mean reward: 0.0095\n",
      "Episode 28200 completed. Mean loss: -0.3249, Mean reward: 0.0090\n",
      "Episode 28300 completed. Mean loss: -0.2072, Mean reward: 0.0030\n",
      "Episode 28400 completed. Mean loss: -0.0616, Mean reward: 0.0100\n",
      "Episode 28500 completed. Mean loss: -0.3522, Mean reward: 0.0090\n",
      "Episode 28600 completed. Mean loss: -0.3511, Mean reward: 0.0040\n",
      "Episode 28700 completed. Mean loss: -0.3006, Mean reward: 0.0170\n",
      "Episode 28800 completed. Mean loss: -0.1921, Mean reward: 0.0030\n",
      "Episode 28900 completed. Mean loss: -0.1211, Mean reward: 0.0095\n",
      "Episode 29000 completed. Mean loss: -0.2844, Mean reward: 0.0100\n",
      "Episode 29100 completed. Mean loss: -0.2590, Mean reward: 0.0090\n",
      "Episode 29200 completed. Mean loss: -0.3601, Mean reward: 0.0110\n",
      "Episode 29300 completed. Mean loss: -0.2016, Mean reward: 0.0050\n",
      "Episode 29400 completed. Mean loss: -0.1849, Mean reward: 0.0030\n",
      "Episode 29500 completed. Mean loss: -0.0812, Mean reward: 0.0095\n",
      "Episode 29600 completed. Mean loss: -0.1090, Mean reward: 0.0215\n",
      "Episode 29700 completed. Mean loss: -0.2320, Mean reward: 0.0090\n",
      "Episode 29800 completed. Mean loss: -0.3222, Mean reward: 0.0090\n",
      "Episode 29900 completed. Mean loss: -0.0812, Mean reward: 0.0090\n",
      "Episode 30000 completed. Mean loss: -0.2324, Mean reward: 0.0160\n",
      "Episode 30100 completed. Mean loss: -0.4528, Mean reward: 0.0095\n",
      "Episode 30200 completed. Mean loss: -0.0181, Mean reward: 0.0060\n",
      "Episode 30300 completed. Mean loss: -0.1759, Mean reward: 0.0075\n",
      "Episode 30400 completed. Mean loss: -0.3327, Mean reward: 0.0115\n",
      "Episode 30500 completed. Mean loss: -0.0466, Mean reward: 0.0115\n",
      "Episode 30600 completed. Mean loss: -0.1254, Mean reward: 0.0120\n",
      "Episode 30700 completed. Mean loss: -0.2730, Mean reward: -0.0010\n",
      "Episode 30800 completed. Mean loss: -0.2544, Mean reward: 0.0190\n",
      "Episode 30900 completed. Mean loss: -0.3754, Mean reward: 0.0160\n",
      "Episode 31000 completed. Mean loss: -0.2772, Mean reward: 0.0030\n",
      "Episode 31100 completed. Mean loss: -0.5678, Mean reward: 0.0190\n",
      "Episode 31200 completed. Mean loss: -0.1522, Mean reward: 0.0155\n",
      "Episode 31300 completed. Mean loss: -0.2703, Mean reward: 0.0090\n",
      "Episode 31400 completed. Mean loss: -0.2522, Mean reward: 0.0050\n",
      "Episode 31500 completed. Mean loss: -0.2939, Mean reward: 0.0195\n",
      "Episode 31600 completed. Mean loss: -0.2560, Mean reward: 0.0125\n",
      "Episode 31700 completed. Mean loss: -0.2777, Mean reward: 0.0175\n",
      "Episode 31800 completed. Mean loss: -0.4466, Mean reward: 0.0060\n",
      "Episode 31900 completed. Mean loss: -0.4043, Mean reward: 0.0135\n",
      "Episode 32000 completed. Mean loss: -0.2949, Mean reward: 0.0080\n",
      "Episode 32100 completed. Mean loss: -0.2130, Mean reward: 0.0140\n",
      "Episode 32200 completed. Mean loss: 0.0378, Mean reward: 0.0150\n",
      "Episode 32300 completed. Mean loss: -0.6995, Mean reward: 0.0115\n",
      "Episode 32400 completed. Mean loss: -0.1043, Mean reward: 0.0135\n",
      "Episode 32500 completed. Mean loss: -0.1325, Mean reward: 0.0170\n",
      "Episode 32600 completed. Mean loss: -0.2219, Mean reward: 0.0095\n",
      "Episode 32700 completed. Mean loss: 0.1876, Mean reward: 0.0135\n",
      "Episode 32800 completed. Mean loss: -0.3588, Mean reward: 0.0070\n",
      "Episode 32900 completed. Mean loss: -0.2784, Mean reward: 0.0180\n",
      "Episode 33000 completed. Mean loss: -0.1105, Mean reward: 0.0060\n",
      "Episode 33100 completed. Mean loss: -0.1064, Mean reward: -0.0045\n",
      "Episode 33200 completed. Mean loss: -0.0492, Mean reward: 0.0070\n",
      "Episode 33300 completed. Mean loss: -0.3710, Mean reward: 0.0160\n",
      "Episode 33400 completed. Mean loss: -0.3111, Mean reward: 0.0140\n",
      "Episode 33500 completed. Mean loss: -0.3984, Mean reward: 0.0230\n",
      "Episode 33600 completed. Mean loss: -0.3160, Mean reward: 0.0075\n",
      "Episode 33700 completed. Mean loss: -0.2210, Mean reward: 0.0155\n",
      "Episode 33800 completed. Mean loss: -0.4329, Mean reward: 0.0050\n",
      "Episode 33900 completed. Mean loss: -0.5407, Mean reward: -0.0005\n",
      "Episode 34000 completed. Mean loss: -0.1532, Mean reward: 0.0075\n",
      "Episode 34100 completed. Mean loss: -0.1979, Mean reward: 0.0090\n",
      "Episode 34200 completed. Mean loss: -0.1462, Mean reward: 0.0015\n",
      "Episode 34300 completed. Mean loss: -0.4633, Mean reward: 0.0015\n",
      "Episode 34400 completed. Mean loss: 0.0274, Mean reward: 0.0150\n",
      "Episode 34500 completed. Mean loss: -0.3123, Mean reward: 0.0175\n",
      "Episode 34600 completed. Mean loss: -0.2249, Mean reward: 0.0100\n",
      "Episode 34700 completed. Mean loss: -0.0194, Mean reward: -0.0025\n",
      "Episode 34800 completed. Mean loss: -0.2133, Mean reward: 0.0135\n",
      "Episode 34900 completed. Mean loss: -0.0464, Mean reward: 0.0110\n",
      "Episode 35000 completed. Mean loss: -0.3184, Mean reward: 0.0135\n",
      "Episode 35100 completed. Mean loss: -0.5606, Mean reward: 0.0115\n",
      "Episode 35200 completed. Mean loss: 0.0508, Mean reward: 0.0115\n",
      "Episode 35300 completed. Mean loss: -0.2698, Mean reward: 0.0155\n",
      "Episode 35400 completed. Mean loss: -0.5108, Mean reward: 0.0130\n",
      "Episode 35500 completed. Mean loss: -0.0793, Mean reward: 0.0115\n",
      "Episode 35600 completed. Mean loss: -0.2855, Mean reward: 0.0150\n",
      "Episode 35700 completed. Mean loss: -0.4612, Mean reward: 0.0115\n",
      "Episode 35800 completed. Mean loss: -0.0347, Mean reward: 0.0120\n",
      "Episode 35900 completed. Mean loss: -0.2453, Mean reward: 0.0075\n",
      "Episode 36000 completed. Mean loss: -0.2008, Mean reward: 0.0140\n",
      "Episode 36100 completed. Mean loss: -0.2313, Mean reward: 0.0140\n",
      "Episode 36200 completed. Mean loss: -0.2047, Mean reward: 0.0075\n",
      "Episode 36300 completed. Mean loss: -0.2600, Mean reward: 0.0060\n",
      "Episode 36400 completed. Mean loss: -0.2753, Mean reward: 0.0110\n",
      "Episode 36500 completed. Mean loss: -0.3615, Mean reward: 0.0150\n",
      "Episode 36600 completed. Mean loss: -0.0902, Mean reward: 0.0070\n",
      "Episode 36700 completed. Mean loss: -0.2464, Mean reward: 0.0115\n",
      "Episode 36800 completed. Mean loss: -0.2881, Mean reward: 0.0050\n",
      "Episode 36900 completed. Mean loss: -0.2147, Mean reward: 0.0135\n",
      "Episode 37000 completed. Mean loss: -0.1689, Mean reward: 0.0135\n",
      "Episode 37100 completed. Mean loss: -0.3920, Mean reward: 0.0135\n",
      "Episode 37200 completed. Mean loss: -0.1674, Mean reward: 0.0070\n",
      "Episode 37300 completed. Mean loss: -0.4225, Mean reward: 0.0030\n",
      "Episode 37400 completed. Mean loss: -0.2818, Mean reward: 0.0215\n",
      "Episode 37500 completed. Mean loss: -0.2027, Mean reward: 0.0155\n",
      "Episode 37600 completed. Mean loss: -0.3227, Mean reward: 0.0170\n",
      "Episode 37700 completed. Mean loss: -0.1783, Mean reward: 0.0200\n",
      "Episode 37800 completed. Mean loss: -0.0558, Mean reward: 0.0015\n",
      "Episode 37900 completed. Mean loss: -0.1628, Mean reward: 0.0060\n",
      "Episode 38000 completed. Mean loss: -0.2827, Mean reward: 0.0095\n",
      "Episode 38100 completed. Mean loss: -0.3856, Mean reward: 0.0130\n",
      "Episode 38200 completed. Mean loss: -0.2261, Mean reward: 0.0125\n",
      "Episode 38300 completed. Mean loss: -0.3652, Mean reward: 0.0080\n",
      "Episode 38400 completed. Mean loss: -0.2176, Mean reward: 0.0095\n",
      "Episode 38500 completed. Mean loss: -0.2831, Mean reward: 0.0075\n",
      "Episode 38600 completed. Mean loss: -0.0390, Mean reward: 0.0080\n",
      "Episode 38700 completed. Mean loss: -0.0650, Mean reward: 0.0075\n",
      "Episode 38800 completed. Mean loss: -0.0785, Mean reward: 0.0090\n",
      "Episode 38900 completed. Mean loss: -0.1822, Mean reward: 0.0095\n",
      "Episode 39000 completed. Mean loss: -0.2765, Mean reward: 0.0015\n",
      "Episode 39100 completed. Mean loss: -0.0580, Mean reward: 0.0170\n",
      "Episode 39200 completed. Mean loss: -0.2907, Mean reward: 0.0070\n",
      "Episode 39300 completed. Mean loss: -0.2878, Mean reward: 0.0050\n",
      "Episode 39400 completed. Mean loss: -0.3141, Mean reward: 0.0115\n",
      "Episode 39500 completed. Mean loss: -0.4169, Mean reward: 0.0130\n",
      "Episode 39600 completed. Mean loss: -0.4086, Mean reward: 0.0050\n",
      "Episode 39700 completed. Mean loss: -0.2994, Mean reward: 0.0135\n",
      "Episode 39800 completed. Mean loss: -0.3396, Mean reward: 0.0130\n",
      "Episode 39900 completed. Mean loss: -0.3328, Mean reward: 0.0055\n",
      "Episode 40000 completed. Mean loss: -0.2025, Mean reward: 0.0135\n",
      "Episode 40100 completed. Mean loss: -0.1073, Mean reward: 0.0095\n",
      "Episode 40200 completed. Mean loss: -0.2824, Mean reward: 0.0060\n",
      "Episode 40300 completed. Mean loss: -0.5284, Mean reward: 0.0035\n",
      "Episode 40400 completed. Mean loss: -0.1815, Mean reward: 0.0115\n",
      "Episode 40500 completed. Mean loss: -0.4505, Mean reward: 0.0110\n",
      "Episode 40600 completed. Mean loss: -0.0310, Mean reward: 0.0070\n",
      "Episode 40700 completed. Mean loss: -0.4997, Mean reward: 0.0140\n",
      "Episode 40800 completed. Mean loss: -0.3239, Mean reward: 0.0115\n",
      "Episode 40900 completed. Mean loss: -0.1490, Mean reward: 0.0075\n",
      "Episode 41000 completed. Mean loss: -0.3528, Mean reward: 0.0080\n",
      "Episode 41100 completed. Mean loss: -0.1197, Mean reward: 0.0070\n",
      "Episode 41200 completed. Mean loss: -0.0276, Mean reward: 0.0130\n",
      "Episode 41300 completed. Mean loss: -0.2137, Mean reward: 0.0060\n",
      "Episode 41400 completed. Mean loss: -0.3041, Mean reward: 0.0035\n",
      "Episode 41500 completed. Mean loss: -0.6809, Mean reward: 0.0130\n",
      "Episode 41600 completed. Mean loss: -0.4551, Mean reward: 0.0150\n",
      "Episode 41700 completed. Mean loss: -0.2246, Mean reward: 0.0090\n",
      "Episode 41800 completed. Mean loss: -0.1900, Mean reward: 0.0175\n",
      "Episode 41900 completed. Mean loss: -0.4280, Mean reward: 0.0145\n",
      "Episode 42000 completed. Mean loss: -0.0814, Mean reward: 0.0055\n",
      "Episode 42100 completed. Mean loss: -0.2434, Mean reward: 0.0125\n",
      "Episode 42200 completed. Mean loss: -0.4410, Mean reward: 0.0120\n",
      "Episode 42300 completed. Mean loss: -0.3019, Mean reward: 0.0135\n",
      "Episode 42400 completed. Mean loss: -0.1675, Mean reward: 0.0090\n",
      "Episode 42500 completed. Mean loss: -0.2036, Mean reward: 0.0015\n",
      "Episode 42600 completed. Mean loss: -0.2606, Mean reward: 0.0115\n",
      "Episode 42700 completed. Mean loss: -0.4524, Mean reward: 0.0115\n",
      "Episode 42800 completed. Mean loss: -0.3177, Mean reward: 0.0055\n",
      "Episode 42900 completed. Mean loss: -0.2910, Mean reward: 0.0175\n",
      "Episode 43000 completed. Mean loss: -0.2242, Mean reward: 0.0115\n",
      "Episode 43100 completed. Mean loss: -0.0981, Mean reward: 0.0110\n",
      "Episode 43200 completed. Mean loss: -0.1894, Mean reward: 0.0130\n",
      "Episode 43300 completed. Mean loss: -0.1750, Mean reward: 0.0095\n",
      "Episode 43400 completed. Mean loss: -0.2682, Mean reward: 0.0115\n",
      "Episode 43500 completed. Mean loss: -0.2326, Mean reward: 0.0105\n",
      "Episode 43600 completed. Mean loss: -0.1357, Mean reward: 0.0095\n",
      "Episode 43700 completed. Mean loss: -0.2637, Mean reward: 0.0110\n",
      "Episode 43800 completed. Mean loss: -0.3644, Mean reward: 0.0140\n",
      "Episode 43900 completed. Mean loss: -0.2943, Mean reward: 0.0190\n",
      "Episode 44000 completed. Mean loss: -0.1790, Mean reward: 0.0130\n",
      "Episode 44100 completed. Mean loss: -0.0978, Mean reward: 0.0110\n",
      "Episode 44200 completed. Mean loss: -0.3922, Mean reward: 0.0015\n",
      "Episode 44300 completed. Mean loss: 0.2760, Mean reward: 0.0135\n",
      "Episode 44400 completed. Mean loss: 0.0494, Mean reward: 0.0095\n",
      "Episode 44500 completed. Mean loss: 0.0936, Mean reward: 0.0090\n",
      "Episode 44600 completed. Mean loss: -0.1040, Mean reward: 0.0090\n",
      "Episode 44700 completed. Mean loss: 0.0708, Mean reward: 0.0035\n",
      "Episode 44800 completed. Mean loss: -0.1144, Mean reward: 0.0070\n",
      "Episode 44900 completed. Mean loss: 0.0153, Mean reward: 0.0095\n",
      "Episode 45000 completed. Mean loss: -0.3995, Mean reward: 0.0140\n",
      "Episode 45100 completed. Mean loss: -0.3145, Mean reward: 0.0095\n",
      "Episode 45200 completed. Mean loss: -0.4136, Mean reward: 0.0070\n",
      "Episode 45300 completed. Mean loss: -0.0408, Mean reward: 0.0080\n",
      "Episode 45400 completed. Mean loss: -0.2777, Mean reward: 0.0010\n",
      "Episode 45500 completed. Mean loss: -0.1278, Mean reward: 0.0115\n",
      "Episode 45600 completed. Mean loss: -0.1620, Mean reward: 0.0070\n",
      "Episode 45700 completed. Mean loss: -0.1973, Mean reward: 0.0130\n",
      "Episode 45800 completed. Mean loss: -0.2223, Mean reward: 0.0050\n",
      "Episode 45900 completed. Mean loss: -0.2843, Mean reward: 0.0110\n",
      "Episode 46000 completed. Mean loss: -0.1652, Mean reward: 0.0110\n",
      "Episode 46100 completed. Mean loss: -0.1546, Mean reward: 0.0055\n",
      "Episode 46200 completed. Mean loss: -0.0450, Mean reward: 0.0180\n",
      "Episode 46300 completed. Mean loss: -0.5075, Mean reward: 0.0135\n",
      "Episode 46400 completed. Mean loss: -0.1864, Mean reward: 0.0040\n",
      "Episode 46500 completed. Mean loss: -0.3840, Mean reward: 0.0115\n",
      "Episode 46600 completed. Mean loss: -0.0270, Mean reward: 0.0155\n",
      "Episode 46700 completed. Mean loss: -0.1810, Mean reward: -0.0010\n",
      "Episode 46800 completed. Mean loss: -0.1895, Mean reward: 0.0095\n",
      "Episode 46900 completed. Mean loss: -0.2856, Mean reward: 0.0070\n",
      "Episode 47000 completed. Mean loss: -0.2846, Mean reward: 0.0115\n",
      "Episode 47100 completed. Mean loss: -0.3226, Mean reward: 0.0035\n",
      "Episode 47200 completed. Mean loss: -0.5245, Mean reward: 0.0050\n",
      "Episode 47300 completed. Mean loss: -0.4249, Mean reward: 0.0155\n",
      "Episode 47400 completed. Mean loss: -0.0764, Mean reward: 0.0095\n",
      "Episode 47500 completed. Mean loss: -0.1889, Mean reward: 0.0060\n",
      "Episode 47600 completed. Mean loss: -0.5733, Mean reward: 0.0150\n",
      "Episode 47700 completed. Mean loss: -0.4752, Mean reward: 0.0155\n",
      "Episode 47800 completed. Mean loss: -0.3904, Mean reward: -0.0005\n",
      "Episode 47900 completed. Mean loss: -0.0785, Mean reward: 0.0230\n",
      "Episode 48000 completed. Mean loss: -0.2236, Mean reward: 0.0050\n",
      "Episode 48100 completed. Mean loss: -0.3863, Mean reward: 0.0115\n",
      "Episode 48200 completed. Mean loss: -0.3166, Mean reward: 0.0120\n",
      "Episode 48300 completed. Mean loss: -0.1268, Mean reward: 0.0135\n",
      "Episode 48400 completed. Mean loss: 0.0403, Mean reward: 0.0035\n",
      "Episode 48500 completed. Mean loss: -0.1243, Mean reward: 0.0010\n",
      "Episode 48600 completed. Mean loss: -0.4066, Mean reward: 0.0130\n",
      "Episode 48700 completed. Mean loss: -0.1287, Mean reward: 0.0105\n",
      "Episode 48800 completed. Mean loss: -0.2783, Mean reward: 0.0140\n",
      "Episode 48900 completed. Mean loss: -0.0289, Mean reward: 0.0070\n",
      "Episode 49000 completed. Mean loss: -0.3741, Mean reward: 0.0090\n",
      "Episode 49100 completed. Mean loss: -0.4529, Mean reward: 0.0110\n",
      "Episode 49200 completed. Mean loss: -0.4807, Mean reward: 0.0135\n",
      "Episode 49300 completed. Mean loss: -0.1586, Mean reward: 0.0050\n",
      "Episode 49400 completed. Mean loss: -0.4187, Mean reward: 0.0010\n",
      "Episode 49500 completed. Mean loss: -0.3649, Mean reward: 0.0075\n",
      "Episode 49600 completed. Mean loss: -0.3865, Mean reward: 0.0175\n",
      "Episode 49700 completed. Mean loss: -0.6020, Mean reward: 0.0015\n",
      "Episode 49800 completed. Mean loss: -0.3889, Mean reward: 0.0055\n",
      "Episode 49900 completed. Mean loss: -0.2601, Mean reward: 0.0040\n",
      "Episode 50000 completed. Mean loss: -0.4186, Mean reward: 0.0215\n",
      "Episode 50100 completed. Mean loss: -0.2769, Mean reward: 0.0130\n",
      "Episode 50200 completed. Mean loss: -0.2547, Mean reward: 0.0055\n",
      "Episode 50300 completed. Mean loss: -0.0587, Mean reward: 0.0135\n",
      "Episode 50400 completed. Mean loss: -0.0414, Mean reward: 0.0080\n",
      "Episode 50500 completed. Mean loss: -0.2790, Mean reward: 0.0075\n",
      "Episode 50600 completed. Mean loss: 0.0373, Mean reward: 0.0115\n",
      "Episode 50700 completed. Mean loss: -0.0828, Mean reward: 0.0125\n",
      "Episode 50800 completed. Mean loss: -0.0479, Mean reward: 0.0110\n",
      "Episode 50900 completed. Mean loss: -0.3376, Mean reward: 0.0140\n",
      "Episode 51000 completed. Mean loss: -0.2931, Mean reward: 0.0090\n",
      "Episode 51100 completed. Mean loss: -0.1431, Mean reward: 0.0115\n",
      "Episode 51200 completed. Mean loss: -0.3464, Mean reward: 0.0190\n",
      "Episode 51300 completed. Mean loss: -0.4175, Mean reward: 0.0135\n",
      "Episode 51400 completed. Mean loss: 0.0193, Mean reward: 0.0115\n",
      "Episode 51500 completed. Mean loss: -0.0694, Mean reward: 0.0140\n",
      "Episode 51600 completed. Mean loss: -0.3533, Mean reward: 0.0090\n",
      "Episode 51700 completed. Mean loss: -0.2546, Mean reward: 0.0070\n",
      "Episode 51800 completed. Mean loss: 0.0642, Mean reward: 0.0080\n",
      "Episode 51900 completed. Mean loss: -0.2536, Mean reward: 0.0135\n",
      "Episode 52000 completed. Mean loss: 0.1768, Mean reward: 0.0110\n",
      "Episode 52100 completed. Mean loss: -0.5406, Mean reward: -0.0010\n",
      "Episode 52200 completed. Mean loss: -0.1417, Mean reward: -0.0070\n",
      "Episode 52300 completed. Mean loss: -0.3211, Mean reward: 0.0090\n",
      "Episode 52400 completed. Mean loss: -0.4375, Mean reward: 0.0060\n",
      "Episode 52500 completed. Mean loss: 0.0731, Mean reward: 0.0100\n",
      "Episode 52600 completed. Mean loss: -0.0656, Mean reward: 0.0035\n",
      "Episode 52700 completed. Mean loss: 0.0861, Mean reward: 0.0090\n",
      "Episode 52800 completed. Mean loss: -0.0899, Mean reward: 0.0095\n",
      "Episode 52900 completed. Mean loss: -0.1162, Mean reward: 0.0095\n",
      "Episode 53000 completed. Mean loss: -0.3682, Mean reward: 0.0110\n",
      "Episode 53100 completed. Mean loss: -0.1191, Mean reward: 0.0015\n",
      "Episode 53200 completed. Mean loss: -0.4211, Mean reward: 0.0110\n",
      "Episode 53300 completed. Mean loss: -0.3214, Mean reward: 0.0060\n",
      "Episode 53400 completed. Mean loss: -0.2283, Mean reward: 0.0135\n",
      "Episode 53500 completed. Mean loss: -0.3329, Mean reward: 0.0180\n",
      "Episode 53600 completed. Mean loss: -0.3752, Mean reward: 0.0120\n",
      "Episode 53700 completed. Mean loss: -0.1207, Mean reward: 0.0180\n",
      "Episode 53800 completed. Mean loss: -0.0057, Mean reward: 0.0020\n",
      "Episode 53900 completed. Mean loss: -0.1823, Mean reward: 0.0175\n",
      "Episode 54000 completed. Mean loss: -0.0448, Mean reward: 0.0130\n",
      "Episode 54100 completed. Mean loss: -0.0978, Mean reward: 0.0235\n",
      "Episode 54200 completed. Mean loss: -0.0810, Mean reward: 0.0170\n",
      "Episode 54300 completed. Mean loss: -0.2800, Mean reward: 0.0100\n",
      "Episode 54400 completed. Mean loss: -0.2563, Mean reward: 0.0130\n",
      "Episode 54500 completed. Mean loss: -0.2773, Mean reward: 0.0235\n",
      "Episode 54600 completed. Mean loss: -0.3125, Mean reward: 0.0150\n",
      "Episode 54700 completed. Mean loss: -0.0416, Mean reward: 0.0135\n",
      "Episode 54800 completed. Mean loss: -0.3886, Mean reward: 0.0155\n",
      "Episode 54900 completed. Mean loss: -0.3764, Mean reward: 0.0115\n",
      "Episode 55000 completed. Mean loss: -0.1868, Mean reward: 0.0010\n",
      "Episode 55100 completed. Mean loss: -0.1892, Mean reward: 0.0020\n",
      "Episode 55200 completed. Mean loss: -0.3308, Mean reward: 0.0090\n",
      "Episode 55300 completed. Mean loss: 0.0424, Mean reward: 0.0140\n",
      "Episode 55400 completed. Mean loss: -0.0626, Mean reward: 0.0150\n",
      "Episode 55500 completed. Mean loss: -0.0124, Mean reward: 0.0055\n",
      "Episode 55600 completed. Mean loss: -0.1578, Mean reward: 0.0090\n",
      "Episode 55700 completed. Mean loss: -0.0947, Mean reward: 0.0050\n",
      "Episode 55800 completed. Mean loss: 0.1152, Mean reward: 0.0135\n",
      "Episode 55900 completed. Mean loss: 0.1535, Mean reward: 0.0110\n",
      "Episode 56000 completed. Mean loss: 0.1932, Mean reward: 0.0135\n",
      "Episode 56100 completed. Mean loss: -0.2559, Mean reward: 0.0055\n",
      "Episode 56200 completed. Mean loss: -0.0936, Mean reward: 0.0035\n",
      "Episode 56300 completed. Mean loss: 0.1076, Mean reward: 0.0130\n",
      "Episode 56400 completed. Mean loss: 0.0018, Mean reward: 0.0115\n",
      "Episode 56500 completed. Mean loss: -0.0699, Mean reward: 0.0130\n",
      "Episode 56600 completed. Mean loss: -0.1684, Mean reward: -0.0025\n",
      "Episode 56700 completed. Mean loss: -0.1086, Mean reward: 0.0135\n",
      "Episode 56800 completed. Mean loss: -0.1644, Mean reward: 0.0055\n",
      "Episode 56900 completed. Mean loss: -0.0941, Mean reward: 0.0030\n",
      "Episode 57000 completed. Mean loss: -0.2088, Mean reward: 0.0095\n",
      "Episode 57100 completed. Mean loss: -0.0564, Mean reward: 0.0010\n",
      "Episode 57200 completed. Mean loss: -0.1420, Mean reward: 0.0170\n",
      "Episode 57300 completed. Mean loss: -0.0314, Mean reward: 0.0035\n",
      "Episode 57400 completed. Mean loss: 0.2365, Mean reward: 0.0030\n",
      "Episode 57500 completed. Mean loss: -0.2081, Mean reward: 0.0130\n",
      "Episode 57600 completed. Mean loss: -0.1270, Mean reward: -0.0025\n",
      "Episode 57700 completed. Mean loss: -0.2565, Mean reward: 0.0195\n",
      "Episode 57800 completed. Mean loss: -0.2256, Mean reward: -0.0005\n",
      "Episode 57900 completed. Mean loss: 0.1691, Mean reward: 0.0000\n",
      "Episode 58000 completed. Mean loss: -0.1712, Mean reward: 0.0030\n",
      "Episode 58100 completed. Mean loss: -0.1326, Mean reward: 0.0050\n",
      "Episode 58200 completed. Mean loss: -0.0172, Mean reward: 0.0055\n",
      "Episode 58300 completed. Mean loss: 0.0046, Mean reward: 0.0170\n",
      "Episode 58400 completed. Mean loss: -0.0244, Mean reward: 0.0015\n",
      "Episode 58500 completed. Mean loss: -0.1873, Mean reward: 0.0095\n",
      "Episode 58600 completed. Mean loss: 0.0149, Mean reward: 0.0070\n",
      "Episode 58700 completed. Mean loss: -0.3530, Mean reward: 0.0095\n",
      "Episode 58800 completed. Mean loss: -0.0252, Mean reward: 0.0120\n",
      "Episode 58900 completed. Mean loss: -0.0885, Mean reward: 0.0070\n",
      "Episode 59000 completed. Mean loss: 0.1643, Mean reward: 0.0070\n",
      "Episode 59100 completed. Mean loss: -0.0649, Mean reward: 0.0150\n",
      "Episode 59200 completed. Mean loss: -0.0195, Mean reward: 0.0115\n",
      "Episode 59300 completed. Mean loss: -0.1876, Mean reward: 0.0030\n",
      "Episode 59400 completed. Mean loss: 0.0702, Mean reward: 0.0150\n",
      "Episode 59500 completed. Mean loss: -0.1888, Mean reward: 0.0045\n",
      "Episode 59600 completed. Mean loss: -0.0563, Mean reward: 0.0140\n",
      "Episode 59700 completed. Mean loss: -0.1058, Mean reward: 0.0075\n",
      "Episode 59800 completed. Mean loss: -0.1498, Mean reward: 0.0070\n",
      "Episode 59900 completed. Mean loss: -0.1465, Mean reward: 0.0230\n",
      "Episode 60000 completed. Mean loss: -0.0707, Mean reward: 0.0075\n",
      "Episode 60100 completed. Mean loss: 0.0300, Mean reward: 0.0035\n",
      "Episode 60200 completed. Mean loss: -0.0838, Mean reward: 0.0195\n",
      "Episode 60300 completed. Mean loss: -0.0926, Mean reward: 0.0110\n",
      "Episode 60400 completed. Mean loss: 0.0394, Mean reward: 0.0075\n",
      "Episode 60500 completed. Mean loss: -0.2883, Mean reward: 0.0120\n",
      "Episode 60600 completed. Mean loss: -0.0774, Mean reward: 0.0035\n",
      "Episode 60700 completed. Mean loss: -0.1290, Mean reward: 0.0135\n",
      "Episode 60800 completed. Mean loss: -0.1667, Mean reward: 0.0095\n",
      "Episode 60900 completed. Mean loss: -0.0948, Mean reward: 0.0110\n",
      "Episode 61000 completed. Mean loss: -0.1732, Mean reward: 0.0200\n",
      "Episode 61100 completed. Mean loss: -0.3595, Mean reward: 0.0150\n",
      "Episode 61200 completed. Mean loss: -0.0563, Mean reward: 0.0055\n",
      "Episode 61300 completed. Mean loss: 0.1221, Mean reward: -0.0005\n",
      "Episode 61400 completed. Mean loss: 0.0690, Mean reward: 0.0055\n",
      "Episode 61500 completed. Mean loss: 0.2431, Mean reward: 0.0090\n",
      "Episode 61600 completed. Mean loss: 0.0636, Mean reward: 0.0155\n",
      "Episode 61700 completed. Mean loss: -0.0054, Mean reward: 0.0110\n",
      "Episode 61800 completed. Mean loss: 0.0506, Mean reward: 0.0190\n",
      "Episode 61900 completed. Mean loss: 0.1435, Mean reward: 0.0180\n",
      "Episode 62000 completed. Mean loss: -0.0703, Mean reward: 0.0190\n",
      "Episode 62100 completed. Mean loss: -0.3078, Mean reward: 0.0040\n",
      "Episode 62200 completed. Mean loss: -0.1254, Mean reward: -0.0010\n",
      "Episode 62300 completed. Mean loss: 0.2484, Mean reward: -0.0050\n",
      "Episode 62400 completed. Mean loss: 0.2364, Mean reward: 0.0110\n",
      "Episode 62500 completed. Mean loss: 0.1963, Mean reward: 0.0160\n",
      "Episode 62600 completed. Mean loss: 0.1644, Mean reward: 0.0100\n",
      "Episode 62700 completed. Mean loss: -0.0947, Mean reward: 0.0270\n",
      "Episode 62800 completed. Mean loss: 0.0781, Mean reward: 0.0150\n",
      "Episode 62900 completed. Mean loss: 0.0758, Mean reward: 0.0175\n",
      "Episode 63000 completed. Mean loss: 0.0595, Mean reward: 0.0140\n",
      "Episode 63100 completed. Mean loss: 0.0325, Mean reward: 0.0160\n",
      "Episode 63200 completed. Mean loss: 0.2475, Mean reward: 0.0155\n",
      "Episode 63300 completed. Mean loss: 0.1565, Mean reward: 0.0170\n",
      "Episode 63400 completed. Mean loss: 0.3179, Mean reward: 0.0175\n",
      "Episode 63500 completed. Mean loss: -0.0734, Mean reward: 0.0225\n",
      "Episode 63600 completed. Mean loss: 0.3844, Mean reward: 0.0080\n",
      "Episode 63700 completed. Mean loss: 0.0039, Mean reward: 0.0170\n",
      "Episode 63800 completed. Mean loss: 0.3046, Mean reward: 0.0110\n",
      "Episode 63900 completed. Mean loss: 0.1802, Mean reward: 0.0070\n",
      "Episode 64000 completed. Mean loss: 0.0730, Mean reward: 0.0095\n",
      "Episode 64100 completed. Mean loss: 0.1155, Mean reward: 0.0150\n",
      "Episode 64200 completed. Mean loss: -0.0462, Mean reward: 0.0090\n",
      "Episode 64300 completed. Mean loss: 0.0670, Mean reward: 0.0115\n",
      "Episode 64400 completed. Mean loss: 0.3099, Mean reward: 0.0120\n",
      "Episode 64500 completed. Mean loss: -0.0086, Mean reward: 0.0170\n",
      "Episode 64600 completed. Mean loss: -0.1985, Mean reward: 0.0100\n",
      "Episode 64700 completed. Mean loss: 0.2362, Mean reward: 0.0130\n",
      "Episode 64800 completed. Mean loss: 0.1084, Mean reward: 0.0030\n",
      "Episode 64900 completed. Mean loss: 0.3403, Mean reward: 0.0080\n",
      "Episode 65000 completed. Mean loss: 0.2848, Mean reward: 0.0115\n",
      "Episode 65100 completed. Mean loss: 0.3431, Mean reward: 0.0075\n",
      "Episode 65200 completed. Mean loss: -0.1076, Mean reward: 0.0035\n",
      "Episode 65300 completed. Mean loss: 0.2402, Mean reward: 0.0090\n",
      "Episode 65400 completed. Mean loss: 0.1644, Mean reward: 0.0130\n",
      "Episode 65500 completed. Mean loss: 0.0921, Mean reward: 0.0110\n",
      "Episode 65600 completed. Mean loss: 0.4008, Mean reward: 0.0140\n",
      "Episode 65700 completed. Mean loss: 0.2805, Mean reward: 0.0050\n",
      "Episode 65800 completed. Mean loss: 0.2714, Mean reward: 0.0035\n",
      "Episode 65900 completed. Mean loss: 0.3380, Mean reward: 0.0195\n",
      "Episode 66000 completed. Mean loss: 0.1923, Mean reward: 0.0090\n",
      "Episode 66100 completed. Mean loss: -0.0503, Mean reward: 0.0035\n",
      "Episode 66200 completed. Mean loss: 0.0556, Mean reward: 0.0110\n",
      "Episode 66300 completed. Mean loss: 0.2773, Mean reward: 0.0030\n",
      "Episode 66400 completed. Mean loss: 0.2510, Mean reward: 0.0030\n",
      "Episode 66500 completed. Mean loss: 0.3067, Mean reward: 0.0195\n",
      "Episode 66600 completed. Mean loss: 0.5514, Mean reward: 0.0120\n",
      "Episode 66700 completed. Mean loss: 0.1637, Mean reward: 0.0035\n",
      "Episode 66800 completed. Mean loss: 0.3661, Mean reward: 0.0030\n",
      "Episode 66900 completed. Mean loss: 0.1272, Mean reward: 0.0095\n",
      "Episode 67000 completed. Mean loss: 0.1576, Mean reward: 0.0090\n",
      "Episode 67100 completed. Mean loss: 0.0773, Mean reward: 0.0135\n",
      "Episode 67200 completed. Mean loss: 0.1411, Mean reward: 0.0130\n",
      "Episode 67300 completed. Mean loss: -0.3605, Mean reward: 0.0155\n",
      "Episode 67400 completed. Mean loss: 0.0843, Mean reward: 0.0155\n",
      "Episode 67500 completed. Mean loss: 0.1255, Mean reward: 0.0090\n",
      "Episode 67600 completed. Mean loss: 0.0689, Mean reward: 0.0150\n",
      "Episode 67700 completed. Mean loss: 0.1784, Mean reward: 0.0130\n",
      "Episode 67800 completed. Mean loss: 0.2399, Mean reward: 0.0130\n",
      "Episode 67900 completed. Mean loss: 0.2214, Mean reward: 0.0100\n",
      "Episode 68000 completed. Mean loss: 0.1146, Mean reward: 0.0120\n",
      "Episode 68100 completed. Mean loss: 0.3126, Mean reward: 0.0130\n",
      "Episode 68200 completed. Mean loss: 0.2328, Mean reward: 0.0060\n",
      "Episode 68300 completed. Mean loss: 0.4461, Mean reward: 0.0205\n",
      "Episode 68400 completed. Mean loss: -0.0767, Mean reward: -0.0005\n",
      "Episode 68500 completed. Mean loss: 0.2393, Mean reward: 0.0070\n",
      "Episode 68600 completed. Mean loss: 0.1674, Mean reward: 0.0175\n",
      "Episode 68700 completed. Mean loss: 0.1540, Mean reward: 0.0075\n",
      "Episode 68800 completed. Mean loss: 0.2329, Mean reward: 0.0100\n",
      "Episode 68900 completed. Mean loss: 0.0956, Mean reward: 0.0195\n",
      "Episode 69000 completed. Mean loss: 0.0410, Mean reward: 0.0210\n",
      "Episode 69100 completed. Mean loss: 0.1717, Mean reward: 0.0120\n",
      "Episode 69200 completed. Mean loss: 0.0865, Mean reward: 0.0055\n",
      "Episode 69300 completed. Mean loss: 0.1091, Mean reward: 0.0035\n",
      "Episode 69400 completed. Mean loss: 0.2897, Mean reward: 0.0090\n",
      "Episode 69500 completed. Mean loss: 0.2048, Mean reward: 0.0005\n",
      "Episode 69600 completed. Mean loss: 0.1674, Mean reward: 0.0150\n",
      "Episode 69700 completed. Mean loss: 0.3471, Mean reward: 0.0165\n",
      "Episode 69800 completed. Mean loss: 0.0503, Mean reward: 0.0160\n",
      "Episode 69900 completed. Mean loss: 0.3977, Mean reward: 0.0130\n",
      "Episode 70000 completed. Mean loss: 0.3541, Mean reward: 0.0165\n",
      "Episode 70100 completed. Mean loss: -0.0394, Mean reward: 0.0060\n",
      "Episode 70200 completed. Mean loss: 0.1398, Mean reward: 0.0070\n",
      "Episode 70300 completed. Mean loss: 0.2494, Mean reward: 0.0110\n",
      "Episode 70400 completed. Mean loss: 0.1328, Mean reward: 0.0060\n",
      "Episode 70500 completed. Mean loss: -0.0566, Mean reward: 0.0175\n",
      "Episode 70600 completed. Mean loss: 0.0196, Mean reward: 0.0080\n",
      "Episode 70700 completed. Mean loss: -0.2545, Mean reward: 0.0115\n",
      "Episode 70800 completed. Mean loss: 0.0011, Mean reward: 0.0090\n",
      "Episode 70900 completed. Mean loss: -0.0024, Mean reward: 0.0175\n",
      "Episode 71000 completed. Mean loss: -0.0455, Mean reward: 0.0050\n",
      "Episode 71100 completed. Mean loss: -0.0407, Mean reward: -0.0005\n",
      "Episode 71200 completed. Mean loss: 0.2468, Mean reward: 0.0115\n",
      "Episode 71300 completed. Mean loss: 0.0138, Mean reward: 0.0140\n",
      "Episode 71400 completed. Mean loss: 0.1530, Mean reward: 0.0130\n",
      "Episode 71500 completed. Mean loss: 0.3847, Mean reward: 0.0180\n",
      "Episode 71600 completed. Mean loss: -0.1259, Mean reward: 0.0050\n",
      "Episode 71700 completed. Mean loss: 0.2580, Mean reward: 0.0050\n",
      "Episode 71800 completed. Mean loss: 0.0028, Mean reward: 0.0055\n",
      "Episode 71900 completed. Mean loss: 0.1279, Mean reward: 0.0090\n",
      "Episode 72000 completed. Mean loss: 0.0664, Mean reward: 0.0010\n",
      "Episode 72100 completed. Mean loss: 0.2471, Mean reward: 0.0135\n",
      "Episode 72200 completed. Mean loss: 0.2540, Mean reward: 0.0135\n",
      "Episode 72300 completed. Mean loss: 0.2390, Mean reward: 0.0075\n",
      "Episode 72400 completed. Mean loss: 0.0609, Mean reward: 0.0095\n",
      "Episode 72500 completed. Mean loss: 0.1624, Mean reward: 0.0190\n",
      "Episode 72600 completed. Mean loss: 0.1630, Mean reward: 0.0075\n",
      "Episode 72700 completed. Mean loss: -0.1090, Mean reward: 0.0160\n",
      "Episode 72800 completed. Mean loss: 0.1663, Mean reward: 0.0110\n",
      "Episode 72900 completed. Mean loss: 0.1934, Mean reward: 0.0080\n",
      "Episode 73000 completed. Mean loss: -0.0798, Mean reward: 0.0130\n",
      "Episode 73100 completed. Mean loss: 0.2897, Mean reward: 0.0120\n",
      "Episode 73200 completed. Mean loss: 0.1726, Mean reward: 0.0115\n",
      "Episode 73300 completed. Mean loss: 0.2614, Mean reward: 0.0095\n",
      "Episode 73400 completed. Mean loss: 0.2278, Mean reward: 0.0015\n",
      "Episode 73500 completed. Mean loss: 0.3257, Mean reward: 0.0040\n",
      "Episode 73600 completed. Mean loss: 0.1007, Mean reward: 0.0070\n",
      "Episode 73700 completed. Mean loss: 0.0740, Mean reward: 0.0015\n",
      "Episode 73800 completed. Mean loss: 0.1345, Mean reward: 0.0115\n",
      "Episode 73900 completed. Mean loss: -0.0002, Mean reward: 0.0120\n",
      "Episode 74000 completed. Mean loss: 0.1769, Mean reward: 0.0160\n",
      "Episode 74100 completed. Mean loss: 0.0382, Mean reward: 0.0175\n",
      "Episode 74200 completed. Mean loss: 0.1590, Mean reward: 0.0095\n",
      "Episode 74300 completed. Mean loss: 0.0763, Mean reward: 0.0110\n",
      "Episode 74400 completed. Mean loss: 0.5323, Mean reward: 0.0190\n",
      "Episode 74500 completed. Mean loss: 0.3394, Mean reward: 0.0200\n",
      "Episode 74600 completed. Mean loss: 0.0711, Mean reward: 0.0160\n",
      "Episode 74700 completed. Mean loss: 0.1835, Mean reward: 0.0110\n",
      "Episode 74800 completed. Mean loss: 0.4601, Mean reward: 0.0130\n",
      "Episode 74900 completed. Mean loss: 0.5904, Mean reward: 0.0170\n",
      "Episode 75000 completed. Mean loss: -0.0183, Mean reward: 0.0175\n",
      "Episode 75100 completed. Mean loss: 0.1850, Mean reward: 0.0130\n",
      "Episode 75200 completed. Mean loss: 0.3392, Mean reward: 0.0090\n",
      "Episode 75300 completed. Mean loss: 0.1543, Mean reward: 0.0170\n",
      "Episode 75400 completed. Mean loss: 0.2283, Mean reward: 0.0110\n",
      "Episode 75500 completed. Mean loss: 0.2991, Mean reward: 0.0170\n",
      "Episode 75600 completed. Mean loss: 0.2782, Mean reward: 0.0110\n",
      "Episode 75700 completed. Mean loss: 0.4036, Mean reward: -0.0005\n",
      "Episode 75800 completed. Mean loss: 0.2969, Mean reward: 0.0070\n",
      "Episode 75900 completed. Mean loss: 0.2315, Mean reward: 0.0130\n",
      "Episode 76000 completed. Mean loss: 0.0417, Mean reward: 0.0040\n",
      "Episode 76100 completed. Mean loss: 0.1568, Mean reward: 0.0090\n",
      "Episode 76200 completed. Mean loss: 0.3411, Mean reward: 0.0115\n",
      "Episode 76300 completed. Mean loss: 0.3872, Mean reward: 0.0135\n",
      "Episode 76400 completed. Mean loss: 0.4320, Mean reward: 0.0115\n",
      "Episode 76500 completed. Mean loss: 0.3363, Mean reward: 0.0070\n",
      "Episode 76600 completed. Mean loss: 0.1764, Mean reward: 0.0140\n",
      "Episode 76700 completed. Mean loss: -0.0659, Mean reward: 0.0040\n",
      "Episode 76800 completed. Mean loss: 0.4586, Mean reward: 0.0040\n",
      "Episode 76900 completed. Mean loss: 0.3720, Mean reward: 0.0175\n",
      "Episode 77000 completed. Mean loss: 0.1932, Mean reward: 0.0150\n",
      "Episode 77100 completed. Mean loss: 0.3520, Mean reward: 0.0035\n",
      "Episode 77200 completed. Mean loss: 0.0072, Mean reward: 0.0215\n",
      "Episode 77300 completed. Mean loss: 0.2942, Mean reward: 0.0050\n",
      "Episode 77400 completed. Mean loss: 0.0489, Mean reward: 0.0100\n",
      "Episode 77500 completed. Mean loss: -0.0384, Mean reward: 0.0070\n",
      "Episode 77600 completed. Mean loss: 0.1718, Mean reward: 0.0075\n",
      "Episode 77700 completed. Mean loss: 0.1677, Mean reward: 0.0090\n",
      "Episode 77800 completed. Mean loss: 0.0928, Mean reward: 0.0135\n",
      "Episode 77900 completed. Mean loss: 0.2106, Mean reward: 0.0160\n",
      "Episode 78000 completed. Mean loss: -0.1245, Mean reward: 0.0155\n",
      "Episode 78100 completed. Mean loss: 0.3584, Mean reward: 0.0050\n",
      "Episode 78200 completed. Mean loss: 0.0790, Mean reward: 0.0070\n",
      "Episode 78300 completed. Mean loss: 0.0111, Mean reward: 0.0180\n",
      "Episode 78400 completed. Mean loss: 0.3454, Mean reward: 0.0035\n",
      "Episode 78500 completed. Mean loss: 0.1971, Mean reward: 0.0115\n",
      "Episode 78600 completed. Mean loss: 0.2079, Mean reward: 0.0195\n",
      "Episode 78700 completed. Mean loss: 0.2493, Mean reward: 0.0075\n",
      "Episode 78800 completed. Mean loss: -0.0106, Mean reward: 0.0080\n",
      "Episode 78900 completed. Mean loss: 0.2238, Mean reward: 0.0140\n",
      "Episode 79000 completed. Mean loss: 0.0054, Mean reward: 0.0100\n",
      "Episode 79100 completed. Mean loss: 0.3382, Mean reward: 0.0115\n",
      "Episode 79200 completed. Mean loss: 0.4407, Mean reward: 0.0115\n",
      "Episode 79300 completed. Mean loss: 0.3250, Mean reward: 0.0155\n",
      "Episode 79400 completed. Mean loss: 0.2170, Mean reward: 0.0130\n",
      "Episode 79500 completed. Mean loss: 0.0769, Mean reward: 0.0100\n",
      "Episode 79600 completed. Mean loss: 0.2760, Mean reward: 0.0035\n",
      "Episode 79700 completed. Mean loss: 0.2308, Mean reward: 0.0115\n",
      "Episode 79800 completed. Mean loss: 0.2650, Mean reward: 0.0155\n",
      "Episode 79900 completed. Mean loss: -0.0360, Mean reward: 0.0055\n",
      "Episode 80000 completed. Mean loss: 0.2498, Mean reward: 0.0170\n",
      "Episode 80100 completed. Mean loss: 0.0322, Mean reward: 0.0065\n",
      "Episode 80200 completed. Mean loss: 0.3075, Mean reward: 0.0055\n",
      "Episode 80300 completed. Mean loss: 0.2707, Mean reward: 0.0095\n",
      "Episode 80400 completed. Mean loss: 0.5085, Mean reward: 0.0210\n",
      "Episode 80500 completed. Mean loss: 0.0746, Mean reward: 0.0135\n",
      "Episode 80600 completed. Mean loss: -0.0719, Mean reward: 0.0075\n",
      "Episode 80700 completed. Mean loss: 0.4285, Mean reward: 0.0075\n",
      "Episode 80800 completed. Mean loss: -0.0316, Mean reward: 0.0070\n",
      "Episode 80900 completed. Mean loss: 0.2387, Mean reward: 0.0060\n",
      "Episode 81000 completed. Mean loss: 0.1923, Mean reward: 0.0035\n",
      "Episode 81100 completed. Mean loss: 0.2725, Mean reward: 0.0095\n",
      "Episode 81200 completed. Mean loss: 0.3512, Mean reward: 0.0095\n",
      "Episode 81300 completed. Mean loss: 0.1896, Mean reward: 0.0155\n",
      "Episode 81400 completed. Mean loss: -0.0080, Mean reward: 0.0035\n",
      "Episode 81500 completed. Mean loss: 0.1600, Mean reward: 0.0215\n",
      "Episode 81600 completed. Mean loss: 0.3284, Mean reward: 0.0130\n",
      "Episode 81700 completed. Mean loss: 0.0632, Mean reward: 0.0115\n",
      "Episode 81800 completed. Mean loss: 0.1847, Mean reward: 0.0055\n",
      "Episode 81900 completed. Mean loss: 0.5592, Mean reward: 0.0095\n",
      "Episode 82000 completed. Mean loss: 0.6066, Mean reward: 0.0135\n",
      "Episode 82100 completed. Mean loss: 0.2118, Mean reward: 0.0020\n",
      "Episode 82200 completed. Mean loss: 0.2370, Mean reward: 0.0210\n",
      "Episode 82300 completed. Mean loss: -0.0386, Mean reward: 0.0150\n",
      "Episode 82400 completed. Mean loss: 0.2255, Mean reward: 0.0050\n",
      "Episode 82500 completed. Mean loss: 0.2869, Mean reward: 0.0155\n",
      "Episode 82600 completed. Mean loss: 0.2915, Mean reward: 0.0195\n",
      "Episode 82700 completed. Mean loss: 0.4227, Mean reward: 0.0130\n",
      "Episode 82800 completed. Mean loss: 0.3520, Mean reward: 0.0115\n",
      "Episode 82900 completed. Mean loss: 0.1271, Mean reward: 0.0180\n",
      "Episode 83000 completed. Mean loss: 0.3857, Mean reward: 0.0095\n",
      "Episode 83100 completed. Mean loss: 0.0127, Mean reward: 0.0150\n",
      "Episode 83200 completed. Mean loss: 0.2456, Mean reward: 0.0100\n",
      "Episode 83300 completed. Mean loss: 0.5337, Mean reward: 0.0075\n",
      "Episode 83400 completed. Mean loss: 0.4658, Mean reward: -0.0025\n",
      "Episode 83500 completed. Mean loss: 0.2389, Mean reward: 0.0040\n",
      "Episode 83600 completed. Mean loss: 0.1838, Mean reward: 0.0130\n",
      "Episode 83700 completed. Mean loss: 0.2668, Mean reward: 0.0050\n",
      "Episode 83800 completed. Mean loss: 0.1956, Mean reward: 0.0130\n",
      "Episode 83900 completed. Mean loss: -0.0468, Mean reward: 0.0095\n",
      "Episode 84000 completed. Mean loss: 0.3838, Mean reward: 0.0075\n",
      "Episode 84100 completed. Mean loss: -0.0560, Mean reward: 0.0075\n",
      "Episode 84200 completed. Mean loss: 0.2873, Mean reward: 0.0110\n",
      "Episode 84300 completed. Mean loss: 0.3671, Mean reward: 0.0050\n",
      "Episode 84400 completed. Mean loss: 0.2373, Mean reward: 0.0075\n",
      "Episode 84500 completed. Mean loss: 0.1419, Mean reward: 0.0030\n",
      "Episode 84600 completed. Mean loss: 0.1079, Mean reward: 0.0040\n",
      "Episode 84700 completed. Mean loss: 0.1152, Mean reward: 0.0110\n",
      "Episode 84800 completed. Mean loss: 0.2685, Mean reward: 0.0090\n",
      "Episode 84900 completed. Mean loss: 0.2097, Mean reward: 0.0035\n",
      "Episode 85000 completed. Mean loss: 0.1583, Mean reward: 0.0155\n",
      "Episode 85100 completed. Mean loss: 0.3199, Mean reward: 0.0055\n",
      "Episode 85200 completed. Mean loss: 0.2108, Mean reward: 0.0075\n",
      "Episode 85300 completed. Mean loss: 0.2011, Mean reward: 0.0190\n",
      "Episode 85400 completed. Mean loss: 0.4567, Mean reward: 0.0055\n",
      "Episode 85500 completed. Mean loss: -0.1641, Mean reward: 0.0130\n",
      "Episode 85600 completed. Mean loss: 0.3898, Mean reward: 0.0110\n",
      "Episode 85700 completed. Mean loss: 0.3008, Mean reward: 0.0100\n",
      "Episode 85800 completed. Mean loss: 0.3420, Mean reward: 0.0180\n",
      "Episode 85900 completed. Mean loss: 0.2972, Mean reward: 0.0075\n",
      "Episode 86000 completed. Mean loss: 0.1901, Mean reward: 0.0135\n",
      "Episode 86100 completed. Mean loss: 0.2293, Mean reward: 0.0110\n",
      "Episode 86200 completed. Mean loss: -0.0613, Mean reward: 0.0115\n",
      "Episode 86300 completed. Mean loss: 0.0976, Mean reward: 0.0095\n",
      "Episode 86400 completed. Mean loss: 0.5689, Mean reward: 0.0100\n",
      "Episode 86500 completed. Mean loss: 0.2430, Mean reward: 0.0075\n",
      "Episode 86600 completed. Mean loss: 0.3662, Mean reward: 0.0095\n",
      "Episode 86700 completed. Mean loss: 0.4571, Mean reward: 0.0075\n",
      "Episode 86800 completed. Mean loss: 0.6088, Mean reward: -0.0005\n",
      "Episode 86900 completed. Mean loss: 0.1842, Mean reward: 0.0075\n",
      "Episode 87000 completed. Mean loss: 0.2389, Mean reward: 0.0015\n",
      "Episode 87100 completed. Mean loss: 0.2462, Mean reward: 0.0210\n",
      "Episode 87200 completed. Mean loss: 0.1073, Mean reward: 0.0080\n",
      "Episode 87300 completed. Mean loss: 0.4657, Mean reward: 0.0110\n",
      "Episode 87400 completed. Mean loss: 0.1968, Mean reward: 0.0175\n",
      "Episode 87500 completed. Mean loss: 0.3528, Mean reward: 0.0115\n",
      "Episode 87600 completed. Mean loss: 0.4597, Mean reward: 0.0190\n",
      "Episode 87700 completed. Mean loss: 0.3796, Mean reward: 0.0035\n",
      "Episode 87800 completed. Mean loss: 0.1805, Mean reward: 0.0095\n",
      "Episode 87900 completed. Mean loss: -0.0061, Mean reward: 0.0095\n",
      "Episode 88000 completed. Mean loss: 0.4729, Mean reward: 0.0095\n",
      "Episode 88100 completed. Mean loss: 0.2845, Mean reward: 0.0130\n",
      "Episode 88200 completed. Mean loss: 0.2518, Mean reward: 0.0070\n",
      "Episode 88300 completed. Mean loss: 0.2552, Mean reward: 0.0055\n",
      "Episode 88400 completed. Mean loss: 0.3812, Mean reward: 0.0090\n",
      "Episode 88500 completed. Mean loss: 0.6534, Mean reward: 0.0115\n",
      "Episode 88600 completed. Mean loss: 0.1389, Mean reward: 0.0110\n",
      "Episode 88700 completed. Mean loss: 0.0067, Mean reward: 0.0075\n",
      "Episode 88800 completed. Mean loss: 0.4015, Mean reward: 0.0110\n",
      "Episode 88900 completed. Mean loss: -0.0148, Mean reward: 0.0115\n",
      "Episode 89000 completed. Mean loss: 0.3408, Mean reward: 0.0115\n",
      "Episode 89100 completed. Mean loss: 0.3154, Mean reward: 0.0040\n",
      "Episode 89200 completed. Mean loss: 0.3592, Mean reward: 0.0055\n",
      "Episode 89300 completed. Mean loss: -0.0298, Mean reward: 0.0115\n",
      "Episode 89400 completed. Mean loss: 0.1488, Mean reward: 0.0150\n",
      "Episode 89500 completed. Mean loss: 0.1437, Mean reward: 0.0080\n",
      "Episode 89600 completed. Mean loss: 0.4572, Mean reward: 0.0095\n",
      "Episode 89700 completed. Mean loss: 0.3056, Mean reward: 0.0055\n",
      "Episode 89800 completed. Mean loss: 0.3894, Mean reward: 0.0150\n",
      "Episode 89900 completed. Mean loss: 0.2301, Mean reward: 0.0040\n",
      "Episode 90000 completed. Mean loss: 0.1451, Mean reward: 0.0100\n",
      "Episode 90100 completed. Mean loss: 0.4720, Mean reward: 0.0015\n",
      "Episode 90200 completed. Mean loss: 0.5461, Mean reward: 0.0050\n",
      "Episode 90300 completed. Mean loss: 0.2469, Mean reward: 0.0075\n",
      "Episode 90400 completed. Mean loss: 0.5814, Mean reward: 0.0010\n",
      "Episode 90500 completed. Mean loss: 0.2165, Mean reward: 0.0135\n",
      "Episode 90600 completed. Mean loss: 0.1672, Mean reward: 0.0110\n",
      "Episode 90700 completed. Mean loss: 0.2430, Mean reward: 0.0130\n",
      "Episode 90800 completed. Mean loss: 0.1485, Mean reward: 0.0210\n",
      "Episode 90900 completed. Mean loss: 0.2023, Mean reward: 0.0175\n",
      "Episode 91000 completed. Mean loss: 0.5728, Mean reward: 0.0150\n",
      "Episode 91100 completed. Mean loss: 0.0493, Mean reward: 0.0135\n",
      "Episode 91200 completed. Mean loss: 0.1055, Mean reward: 0.0145\n",
      "Episode 91300 completed. Mean loss: 0.1271, Mean reward: 0.0145\n",
      "Episode 91400 completed. Mean loss: 0.3607, Mean reward: 0.0050\n",
      "Episode 91500 completed. Mean loss: 0.0863, Mean reward: 0.0255\n",
      "Episode 91600 completed. Mean loss: 0.3876, Mean reward: 0.0140\n",
      "Episode 91700 completed. Mean loss: 0.1198, Mean reward: 0.0130\n",
      "Episode 91800 completed. Mean loss: 0.2785, Mean reward: 0.0075\n",
      "Episode 91900 completed. Mean loss: 0.0855, Mean reward: 0.0015\n",
      "Episode 92000 completed. Mean loss: 0.0350, Mean reward: 0.0170\n",
      "Episode 92100 completed. Mean loss: 0.2745, Mean reward: 0.0055\n",
      "Episode 92200 completed. Mean loss: 0.1322, Mean reward: 0.0095\n",
      "Episode 92300 completed. Mean loss: 0.4288, Mean reward: 0.0070\n",
      "Episode 92400 completed. Mean loss: -0.0006, Mean reward: 0.0075\n",
      "Episode 92500 completed. Mean loss: 0.2472, Mean reward: 0.0200\n",
      "Episode 92600 completed. Mean loss: 0.1355, Mean reward: 0.0085\n",
      "Episode 92700 completed. Mean loss: 0.0505, Mean reward: 0.0115\n",
      "Episode 92800 completed. Mean loss: 0.0706, Mean reward: 0.0115\n",
      "Episode 92900 completed. Mean loss: 0.1171, Mean reward: 0.0090\n",
      "Episode 93000 completed. Mean loss: 0.0764, Mean reward: 0.0125\n",
      "Episode 93100 completed. Mean loss: 0.4577, Mean reward: 0.0115\n",
      "Episode 93200 completed. Mean loss: 0.2942, Mean reward: 0.0135\n",
      "Episode 93300 completed. Mean loss: 0.0716, Mean reward: 0.0130\n",
      "Episode 93400 completed. Mean loss: 0.4274, Mean reward: 0.0095\n",
      "Episode 93500 completed. Mean loss: 0.3589, Mean reward: 0.0110\n",
      "Episode 93600 completed. Mean loss: 0.1878, Mean reward: 0.0070\n",
      "Episode 93700 completed. Mean loss: 0.3604, Mean reward: 0.0155\n",
      "Episode 93800 completed. Mean loss: -0.0174, Mean reward: 0.0120\n",
      "Episode 93900 completed. Mean loss: 0.1654, Mean reward: 0.0170\n",
      "Episode 94000 completed. Mean loss: 0.3717, Mean reward: 0.0110\n",
      "Episode 94100 completed. Mean loss: 0.0913, Mean reward: 0.0195\n",
      "Episode 94200 completed. Mean loss: 0.1249, Mean reward: 0.0140\n",
      "Episode 94300 completed. Mean loss: 0.2449, Mean reward: 0.0115\n",
      "Episode 94400 completed. Mean loss: 0.1217, Mean reward: 0.0140\n",
      "Episode 94500 completed. Mean loss: 0.2710, Mean reward: 0.0095\n",
      "Episode 94600 completed. Mean loss: 0.3078, Mean reward: 0.0160\n",
      "Episode 94700 completed. Mean loss: 0.3276, Mean reward: 0.0050\n",
      "Episode 94800 completed. Mean loss: 0.4894, Mean reward: 0.0070\n",
      "Episode 94900 completed. Mean loss: 0.1269, Mean reward: 0.0165\n",
      "Episode 95000 completed. Mean loss: 0.0657, Mean reward: 0.0110\n",
      "Episode 95100 completed. Mean loss: 0.0944, Mean reward: 0.0150\n",
      "Episode 95200 completed. Mean loss: -0.1746, Mean reward: 0.0050\n",
      "Episode 95300 completed. Mean loss: 0.0157, Mean reward: 0.0075\n",
      "Episode 95400 completed. Mean loss: -0.0234, Mean reward: 0.0130\n",
      "Episode 95500 completed. Mean loss: 0.1159, Mean reward: 0.0140\n",
      "Episode 95600 completed. Mean loss: 0.3768, Mean reward: 0.0130\n",
      "Episode 95700 completed. Mean loss: 0.1107, Mean reward: 0.0130\n",
      "Episode 95800 completed. Mean loss: -0.1110, Mean reward: 0.0075\n",
      "Episode 95900 completed. Mean loss: 0.1082, Mean reward: 0.0030\n",
      "Episode 96000 completed. Mean loss: 0.3272, Mean reward: 0.0090\n",
      "Episode 96100 completed. Mean loss: 0.0202, Mean reward: 0.0070\n",
      "Episode 96200 completed. Mean loss: 0.0590, Mean reward: 0.0090\n",
      "Episode 96300 completed. Mean loss: 0.1318, Mean reward: 0.0075\n",
      "Episode 96400 completed. Mean loss: 0.0536, Mean reward: 0.0050\n",
      "Episode 96500 completed. Mean loss: 0.2658, Mean reward: -0.0010\n",
      "Episode 96600 completed. Mean loss: -0.0101, Mean reward: 0.0070\n",
      "Episode 96700 completed. Mean loss: -0.0360, Mean reward: 0.0135\n",
      "Episode 96800 completed. Mean loss: -0.1210, Mean reward: 0.0090\n",
      "Episode 96900 completed. Mean loss: 0.2638, Mean reward: 0.0075\n",
      "Episode 97000 completed. Mean loss: 0.1793, Mean reward: 0.0115\n",
      "Episode 97100 completed. Mean loss: 0.1084, Mean reward: 0.0025\n",
      "Episode 97200 completed. Mean loss: -0.0003, Mean reward: 0.0040\n",
      "Episode 97300 completed. Mean loss: 0.5323, Mean reward: 0.0155\n",
      "Episode 97400 completed. Mean loss: 0.3772, Mean reward: 0.0090\n",
      "Episode 97500 completed. Mean loss: 0.3106, Mean reward: 0.0075\n",
      "Episode 97600 completed. Mean loss: -0.2054, Mean reward: 0.0150\n",
      "Episode 97700 completed. Mean loss: 0.0485, Mean reward: 0.0140\n",
      "Episode 97800 completed. Mean loss: 0.3925, Mean reward: 0.0140\n",
      "Episode 97900 completed. Mean loss: 0.2305, Mean reward: 0.0050\n",
      "Episode 98000 completed. Mean loss: -0.0206, Mean reward: 0.0195\n",
      "Episode 98100 completed. Mean loss: 0.0017, Mean reward: 0.0110\n",
      "Episode 98200 completed. Mean loss: 0.0641, Mean reward: 0.0100\n",
      "Episode 98300 completed. Mean loss: 0.0424, Mean reward: 0.0055\n",
      "Episode 98400 completed. Mean loss: 0.3497, Mean reward: 0.0115\n",
      "Episode 98500 completed. Mean loss: 0.1442, Mean reward: 0.0130\n",
      "Episode 98600 completed. Mean loss: 0.2508, Mean reward: 0.0140\n",
      "Episode 98700 completed. Mean loss: 0.2329, Mean reward: 0.0080\n",
      "Episode 98800 completed. Mean loss: 0.3982, Mean reward: 0.0115\n",
      "Episode 98900 completed. Mean loss: 0.4275, Mean reward: 0.0060\n",
      "Episode 99000 completed. Mean loss: 0.4751, Mean reward: 0.0075\n",
      "Episode 99100 completed. Mean loss: 0.4377, Mean reward: 0.0125\n",
      "Episode 99200 completed. Mean loss: 0.1824, Mean reward: 0.0155\n",
      "Episode 99300 completed. Mean loss: 0.3515, Mean reward: 0.0195\n",
      "Episode 99400 completed. Mean loss: 0.4107, Mean reward: 0.0155\n",
      "Episode 99500 completed. Mean loss: 0.1944, Mean reward: 0.0045\n",
      "Episode 99600 completed. Mean loss: 0.5823, Mean reward: 0.0175\n",
      "Episode 99700 completed. Mean loss: 0.3146, Mean reward: 0.0055\n",
      "Episode 99800 completed. Mean loss: 0.3577, Mean reward: 0.0030\n",
      "Episode 99900 completed. Mean loss: 0.2185, Mean reward: 0.0080\n",
      "Training completed in 21228.92 seconds\n",
      "The agent is ready to play Oomi!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Adjustable variables\n",
    "NUM_EPISODES = 100000\n",
    "NUM_AGENTS = 4\n",
    "HIDDEN_DIM = 64\n",
    "BATCH_SIZE = 64\n",
    "REPLAY_BUFFER_CAPACITY = 10000\n",
    "LEARNING_RATE = 0.001\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.01\n",
    "EPSILON_DECAY = 0.995\n",
    "\n",
    "# Model saving and loading paths\n",
    "MODEL_SAVE_PATH = \"/kaggle/working/models/\"\n",
    "CSV_SAVE_PATH = \"/kaggle/working/csvs/\"\n",
    "BEST_MODEL_PATH = os.path.join(MODEL_SAVE_PATH, \"best_model.pth\")\n",
    "BACKUP_MODEL_PATH = os.path.join(MODEL_SAVE_PATH, \"backup_model.pth\")\n",
    "CSV_FILE_PATH = os.path.join(CSV_SAVE_PATH, \"training_stats.csv\")\n",
    "\n",
    "# Create model directory if it doesn't exist\n",
    "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "os.makedirs(CSV_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# Training control\n",
    "CONTINUE_TRAINING = True  # Set to True to continue training after loading\n",
    "ADDITIONAL_EPISODES = 100000  # Number of additional episodes to train if continuing\n",
    "\n",
    "\n",
    "# Constants (unchanged)\n",
    "SUITS = ['Hearts', 'Diamonds', 'Clubs', 'Spades']\n",
    "VALUES = ['7', '8', '9', '10', 'J', 'Q', 'K', 'A']\n",
    "NUM_PLAYERS = 4\n",
    "HAND_SIZE = 8\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class Card:\n",
    "    def __init__(self, suit, value):\n",
    "        self.suit = suit\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.value} of {self.suit}\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.suit == other.suit and self.value == other.value\n",
    "\n",
    "    def to_index(self):\n",
    "        return SUITS.index(self.suit) * len(VALUES) + VALUES.index(self.value)\n",
    "    \n",
    "class OomiGame:\n",
    "    def __init__(self):\n",
    "        self.deck = self.create_deck()\n",
    "        self.hands = self.deal_cards()\n",
    "        self.played_cards = []\n",
    "        self.current_player = 0\n",
    "        self.current_trick = []\n",
    "        self.tricks_won = {'team1': 0, 'team2': 0}\n",
    "        self.trump_suit = None\n",
    "\n",
    "    def create_deck(self):\n",
    "        return [Card(suit, value) for suit in SUITS for value in VALUES]\n",
    "\n",
    "    def deal_cards(self):\n",
    "        random.shuffle(self.deck)\n",
    "        return [self.deck[i::NUM_PLAYERS] for i in range(NUM_PLAYERS)]\n",
    "\n",
    "    def choose_trump(self, suit):\n",
    "        self.trump_suit = suit\n",
    "\n",
    "    def play_move(self, card):\n",
    "        if card not in self.hands[self.current_player]:\n",
    "            raise ValueError(\"Invalid move: card not in player's hand\")\n",
    "\n",
    "        self.hands[self.current_player].remove(card)\n",
    "        self.current_trick.append(card)\n",
    "        self.played_cards.append(card)\n",
    "\n",
    "        if len(self.current_trick) == NUM_PLAYERS:\n",
    "            winning_card = self.determine_trick_winner()\n",
    "            winning_player = self.current_trick.index(winning_card)\n",
    "            winning_team = 'team1' if winning_player in [0, 2] else 'team2'\n",
    "            self.tricks_won[winning_team] += 1\n",
    "            self.current_player = winning_player\n",
    "            self.current_trick = []\n",
    "        else:\n",
    "            self.current_player = (self.current_player + 1) % NUM_PLAYERS\n",
    "\n",
    "    def determine_trick_winner(self):\n",
    "        lead_suit = self.current_trick[0].suit\n",
    "        trump_cards = [card for card in self.current_trick if card.suit == self.trump_suit]\n",
    "        if trump_cards:\n",
    "            return max(trump_cards, key=lambda card: VALUES.index(card.value))\n",
    "        else:\n",
    "            return max((card for card in self.current_trick if card.suit == lead_suit), \n",
    "                       key=lambda card: VALUES.index(card.value))\n",
    "\n",
    "    def get_state(self):\n",
    "        return {\n",
    "            'hand': self.hands[self.current_player].copy(),\n",
    "            'desk': self.current_trick.copy(),\n",
    "            'played': self.played_cards.copy(),\n",
    "            'current_player': self.current_player,\n",
    "            'tricks_won': self.tricks_won.copy(),\n",
    "            'trump_suit': self.trump_suit\n",
    "        }\n",
    "\n",
    "    def is_game_over(self):\n",
    "        return all(len(hand) == 0 for hand in self.hands)\n",
    "\n",
    "    def get_winner(self):\n",
    "        if self.tricks_won['team1'] > self.tricks_won['team2']:\n",
    "            return 'team1'\n",
    "        elif self.tricks_won['team2'] > self.tricks_won['team1']:\n",
    "            return 'team2'\n",
    "        else:\n",
    "            return 'tie'\n",
    "        \n",
    "        \n",
    "class CardGameState(nn.Module):\n",
    "    def __init__(self, num_suits, num_ranks, hidden_dim):\n",
    "        super(CardGameState, self).__init__()\n",
    "        self.num_suits = num_suits\n",
    "        self.num_ranks = num_ranks\n",
    "        self.card_embedding = nn.Embedding(num_suits * num_ranks, hidden_dim)\n",
    "        self.suit_embedding = nn.Embedding(num_suits + 1, hidden_dim)  # +1 for no trump\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim, 4)\n",
    "        \n",
    "        # Add a final layer to combine all states\n",
    "        self.combine_layer = nn.Linear(hidden_dim * 4, hidden_dim)\n",
    "\n",
    "    def forward(self, trump_suit, hand, desk, played):\n",
    "        # Embed trump suit\n",
    "        trump_embed = self.suit_embedding(trump_suit).unsqueeze(1)\n",
    "        \n",
    "        # Process hand, desk, and played cards\n",
    "        hand_state = self._process_cards(hand)\n",
    "        desk_state = self._process_cards(desk)\n",
    "        played_state = self._process_cards(played)\n",
    "        \n",
    "        # Combine all states\n",
    "        game_state = torch.cat([trump_embed, hand_state, desk_state, played_state], dim=1)\n",
    "        return self.combine_layer(game_state.view(game_state.size(0), -1))\n",
    "\n",
    "    def _process_cards(self, cards):\n",
    "        embedded = self.card_embedding(cards)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        return attn_out.mean(1).unsqueeze(1)\n",
    "    \n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_cards):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, num_cards)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, game_state, valid_actions):\n",
    "        x = F.relu(self.fc1(game_state))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc3(x)\n",
    "        \n",
    "        # Apply action mask\n",
    "        logits = logits.masked_fill(~valid_actions, float('-inf'))\n",
    "        return F.softmax(logits, dim=-1)\n",
    "    \n",
    "    \n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "\n",
    "class RandomPlayer:\n",
    "    def random_move(self, state):\n",
    "        return random.choice(state['hand'])\n",
    "    \n",
    "    \n",
    "class OomiAgent:\n",
    "    def __init__(self, num_suits, num_ranks, hidden_dim):\n",
    "        self.num_suits = num_suits\n",
    "        self.num_ranks = num_ranks\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_cards = num_suits * num_ranks\n",
    "        self.game_state_model = CardGameState(num_suits, num_ranks, hidden_dim).to(device)\n",
    "        self.policy_network = PolicyNetwork(hidden_dim, hidden_dim, self.num_cards).to(device)\n",
    "        self.optimizer = optim.Adam(\n",
    "            list(self.game_state_model.parameters()) + \n",
    "            list(self.policy_network.parameters()),\n",
    "            lr=LEARNING_RATE\n",
    "        )\n",
    "        self.epsilon = EPSILON_START\n",
    "        self.total_training_time = 0\n",
    "        self.total_games_played = 0\n",
    "        self.best_loss = float('inf')\n",
    "\n",
    "    def state_to_tensor(self, state):\n",
    "        trump_suit = torch.tensor([SUITS.index(state['trump_suit']) if state['trump_suit'] else len(SUITS)], device=device)\n",
    "        hand = torch.tensor([card.to_index() for card in state['hand']], dtype=torch.long, device=device)\n",
    "        desk = torch.tensor([card.to_index() for card in state['desk']], dtype=torch.long, device=device)\n",
    "        played = torch.tensor([card.to_index() for card in state['played']], dtype=torch.long, device=device)\n",
    "\n",
    "        # Pad sequences to fixed length\n",
    "        hand = F.pad(hand, (0, HAND_SIZE - len(hand)))\n",
    "        desk = F.pad(desk, (0, NUM_PLAYERS - len(desk)))\n",
    "        played = F.pad(played, (0, self.num_suits * self.num_ranks - len(played)))\n",
    "        \n",
    "        return trump_suit, hand.unsqueeze(0), desk.unsqueeze(0), played.unsqueeze(0)\n",
    "\n",
    "    \n",
    "    def get_valid_actions(self, state):\n",
    "        valid_actions = torch.zeros(self.num_cards, dtype=torch.bool, device=device)\n",
    "        \n",
    "        c_hand_suit = state['desk'][0].suit if len(state['desk'])>0 else None\n",
    "        \n",
    "        if any([c_hand_suit == c.suit for c in state['hand']]):\n",
    "            for card in state['hand']:\n",
    "                if c_hand_suit == card.suit:\n",
    "                    valid_actions[card.to_index()] = True\n",
    "        else:\n",
    "            for card in state['hand']:\n",
    "                valid_actions[card.to_index()] = True\n",
    "                \n",
    "        return valid_actions.unsqueeze(0)\n",
    "\n",
    "    def predict(self, state):\n",
    "        trump_suit, hand, desk, played = self.state_to_tensor(state)\n",
    "        valid_actions = self.get_valid_actions(state)\n",
    "        game_state = self.game_state_model(trump_suit, hand, desk, played)        \n",
    "        action_probs = self.policy_network(game_state, valid_actions)\n",
    "        return action_probs.squeeze(0).cpu().detach().numpy()\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        action_probs = self.predict(state)\n",
    "       \n",
    "        # Choose action based on the probability distribution\n",
    "        chosen_card_index = np.random.choice(len(action_probs), p=action_probs)\n",
    "        \n",
    "        # Convert chosen index back to Card object\n",
    "        suit_index = chosen_card_index // len(VALUES)\n",
    "        value_index = chosen_card_index % len(VALUES)\n",
    "        return Card(SUITS[suit_index], VALUES[value_index])\n",
    "    \n",
    "    def evaluate(self, num_games=100):\n",
    "        wins = 0\n",
    "        for _ in range(num_games):\n",
    "            game = OomiGame()\n",
    "            players = [self if i % 2 == 0 else RandomPlayer() for i in range(NUM_PLAYERS)]\n",
    "\n",
    "            while not game.is_game_over():\n",
    "                current_player = players[game.current_player]\n",
    "                state = game.get_state()\n",
    "                action = current_player.choose_action(state) if isinstance(current_player, OomiAgent) else current_player.random_move(state)\n",
    "                game.play_move(action)\n",
    "\n",
    "            if game.get_winner() == 'team1':  # Assuming self plays as team1\n",
    "                wins += 1\n",
    "        \n",
    "        win_rate = wins / num_games\n",
    "        \n",
    "        print(f'win rate - {win_rate}')\n",
    "\n",
    "        return win_rate  # Return win rate\n",
    "\n",
    "\n",
    "    def perturb(self):\n",
    "        with torch.no_grad():\n",
    "            for param in self.game_state_model.parameters():\n",
    "                param.add_(torch.randn(param.size()) * 0.1)\n",
    "            for param in self.policy_network.parameters():\n",
    "                param.add_(torch.randn(param.size()) * 0.1)\n",
    "\n",
    "    def train(self, states, actions, rewards):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Data preparation\n",
    "        trump_suits, hands, desks, playeds = zip(*[self.state_to_tensor(state) for state in states])\n",
    "        trump_suits = torch.cat(trump_suits)\n",
    "        hands = torch.cat(hands)\n",
    "        desks = torch.cat(desks)\n",
    "        playeds = torch.cat(playeds)\n",
    "        valid_actions = torch.cat([self.get_valid_actions(state) for state in states])\n",
    "\n",
    "        # Forward pass\n",
    "        game_states = self.game_state_model(trump_suits, hands, desks, playeds)\n",
    "        action_probs = self.policy_network(game_states, valid_actions)\n",
    "\n",
    "        # Calculate loss\n",
    "        action_indices = torch.tensor([action.to_index() for action in actions], device=device)\n",
    "        rewards_tensor = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "\n",
    "        # Compute baseline (mean reward for the batch)\n",
    "        baseline = rewards_tensor.mean()\n",
    "\n",
    "        # Normalize rewards\n",
    "        if rewards_tensor.std() > 1e-8:\n",
    "            rewards_tensor = (rewards_tensor - rewards_tensor.mean()) / (rewards_tensor.std() + 1e-8)\n",
    "        else:\n",
    "            rewards_tensor = rewards_tensor - rewards_tensor.mean()\n",
    "\n",
    "        # Compute advantages\n",
    "        advantages = rewards_tensor - baseline\n",
    "\n",
    "        # Policy gradient loss using REINFORCE with baseline\n",
    "        log_probs = torch.log(action_probs + 1e-8)\n",
    "        policy_loss = -(log_probs[torch.arange(len(actions)), action_indices] * advantages).mean()\n",
    "\n",
    "        # Entropy regularization\n",
    "        entropy = -(action_probs * log_probs).sum(dim=-1).mean()\n",
    "\n",
    "        # Total loss (including entropy regularization)\n",
    "        loss = policy_loss - 0.01 * entropy\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(self.game_state_model.parameters(), max_norm=1.0)\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_network.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Optimization step\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save({\n",
    "            'game_state_model': self.game_state_model.state_dict(),\n",
    "            'policy_network': self.policy_network.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'epsilon': self.epsilon,\n",
    "            'total_training_time': self.total_training_time,\n",
    "            'total_games_played': self.total_games_played,\n",
    "            'best_loss': self.best_loss\n",
    "        }, path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        checkpoint = torch.load(path, map_location=torch.device(device))\n",
    "        self.game_state_model.load_state_dict(checkpoint['game_state_model'])\n",
    "        self.policy_network.load_state_dict(checkpoint['policy_network'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.epsilon = checkpoint['epsilon']\n",
    "        self.total_training_time = checkpoint['total_training_time']\n",
    "        self.total_games_played = checkpoint['total_games_played']\n",
    "        self.best_loss = checkpoint['best_loss']\n",
    "\n",
    "def self_play_training(num_episodes, starting_agent=None):\n",
    "    if starting_agent:\n",
    "        agents = [starting_agent] + [OomiAgent(len(SUITS), len(VALUES), HIDDEN_DIM) for _ in range(NUM_AGENTS - 1)]\n",
    "    else:\n",
    "        agents = [OomiAgent(len(SUITS), len(VALUES), HIDDEN_DIM) for _ in range(NUM_AGENTS)]\n",
    "    \n",
    "    replay_buffer = ReplayBuffer(REPLAY_BUFFER_CAPACITY)\n",
    "    \n",
    "    episode_losses = []\n",
    "    episode_rewards = []\n",
    "    start_time = time.time()\n",
    "    best_agent = agents[0]\n",
    "    loss_increase_count = 0\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        game = OomiGame()\n",
    "        player_agents = random.sample(agents, NUM_PLAYERS)\n",
    "        \n",
    "        # Choose trump suit\n",
    "        first_player_hand = game.hands[0][:4]  # First 4 cards of the first player\n",
    "        trump_suit = player_agents[0].choose_action({'hand': first_player_hand, 'desk': [], 'played': [],\n",
    "                                                     'current_player': 0, 'tricks_won': game.tricks_won,\n",
    "                                                     'trump_suit': None}).suit\n",
    "        game.choose_trump(trump_suit)\n",
    "        \n",
    "        data = []\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not game.is_game_over():\n",
    "            state = game.get_state()\n",
    "            player_id = game.current_player\n",
    "            current_agent = player_agents[game.current_player]\n",
    "            \n",
    "            if random.random() < current_agent.epsilon:\n",
    "                action = random.choice(state['hand'])\n",
    "            else:\n",
    "                action = current_agent.choose_action(state)\n",
    "            \n",
    "            prev_tricks_won = game.tricks_won.copy()\n",
    "            game.play_move(action)\n",
    "            \n",
    "            new_state = game.get_state()\n",
    "            \n",
    "            # Intermediate reward\n",
    "            if game.tricks_won['team1'] > prev_tricks_won['team1']:\n",
    "                reward = 0.1 if player_id in [0, 2] else -0.1\n",
    "            elif game.tricks_won['team2'] > prev_tricks_won['team2']:\n",
    "                reward = 0.1 if player_id in [1, 3] else -0.1\n",
    "            else:\n",
    "                reward = 0\n",
    "            \n",
    "            # Bonus for playing trump\n",
    "            if action.suit == game.trump_suit:\n",
    "                reward += 0.05\n",
    "                \n",
    "            data.append((state, action, reward, new_state, player_id))\n",
    "        \n",
    "        # Game over, assign final rewards\n",
    "        winner = game.get_winner()\n",
    "        for experience in data:\n",
    "            state, action, intermediate_reward, new_state, player = experience\n",
    "            if winner == 'team1':\n",
    "                final_reward = 1 if player in [0, 2] else -1\n",
    "            elif winner == 'team2':\n",
    "                final_reward = 1 if player in [1, 3] else -1\n",
    "            else:  # tie\n",
    "                final_reward = 0\n",
    "            \n",
    "            # Scale final reward based on margin of victory\n",
    "            margin = abs(game.tricks_won['team1'] - game.tricks_won['team2'])\n",
    "            final_reward *= (1 + 0.1 * margin)\n",
    "            \n",
    "            # Combine intermediate and final rewards\n",
    "            total_reward = intermediate_reward + final_reward\n",
    "            replay_buffer.add((state, action, total_reward, new_state, player))\n",
    "            episode_rewards.append(total_reward)\n",
    "        \n",
    "        # Training phase\n",
    "        if len(replay_buffer.buffer) >= BATCH_SIZE:\n",
    "            for agent in agents:\n",
    "                batch = replay_buffer.sample(BATCH_SIZE)\n",
    "                states, actions, rewards, _, _ = zip(*batch)\n",
    "                loss = agent.train(states, actions, rewards)\n",
    "                episode_losses.append(loss)\n",
    "                \n",
    "                # Update best agent if this agent has lower loss\n",
    "                if loss < best_agent.best_loss:\n",
    "                    best_agent = agent\n",
    "                    best_agent.best_loss = loss\n",
    "                    best_agent.save_model(BEST_MODEL_PATH)\n",
    "                    loss_increase_count = 0\n",
    "                else:\n",
    "                    loss_increase_count += 1\n",
    "                            \n",
    "        # Update epsilon\n",
    "        for agent in agents:\n",
    "            agent.epsilon = max(EPSILON_END, agent.epsilon * EPSILON_DECAY)\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            best_agent.save_model(BEST_MODEL_PATH)\n",
    "            mean_loss = np.mean(episode_losses[-100:])\n",
    "            mean_reward = np.mean(episode_rewards[-100:])\n",
    "            print(f\"Episode {episode} completed. Mean loss: {mean_loss:.4f}, Mean reward: {mean_reward:.4f}\")\n",
    "        \n",
    "        # Backup model if loss increases continuously\n",
    "        if loss_increase_count >= 50:\n",
    "            best_agent.save_model(BACKUP_MODEL_PATH)\n",
    "#             print(f\"Backup model saved due to continuous loss increase.\")\n",
    "            loss_increase_count = 0\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_training_time = end_time - start_time\n",
    "    \n",
    "    # Save training statistics\n",
    "    with open(CSV_FILE_PATH, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Episode', 'Mean Loss', 'Mean Reward'])\n",
    "        for i in range(num_episodes):\n",
    "            writer.writerow([i, np.mean(episode_losses[max(0, i-10):i+1]), np.mean(episode_rewards[max(0, i-10):i+1])])\n",
    "    \n",
    "    print(f\"Training completed in {total_training_time:.2f} seconds\")\n",
    "    return best_agent\n",
    "\n",
    "def load_or_train_model():\n",
    "    if os.path.exists('/kaggle/input/rl-oomicardmodel/pytorch/new2.0.0/1/models/best_model.pth'):\n",
    "        print(\"Loading existing model...\")\n",
    "        agent = OomiAgent(len(SUITS), len(VALUES), HIDDEN_DIM)\n",
    "        agent.load_model('/kaggle/input/rl-oomicardmodel/pytorch/new2.0.0/1/models/best_model.pth')\n",
    "        if CONTINUE_TRAINING:\n",
    "            print(f\"Continuing training for {ADDITIONAL_EPISODES} more episodes...\")\n",
    "            agent = self_play_training(ADDITIONAL_EPISODES, starting_agent=agent)\n",
    "        return agent\n",
    "    else:\n",
    "        print(\"No existing model found. Starting new training...\")\n",
    "        return self_play_training(NUM_EPISODES)\n",
    "\n",
    "# Main execution\n",
    "best_agent = load_or_train_model()\n",
    "print(\"The agent is ready to play Oomi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b87344ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T10:44:46.529808Z",
     "iopub.status.busy": "2024-07-14T10:44:46.528961Z",
     "iopub.status.idle": "2024-07-14T10:44:46.533946Z",
     "shell.execute_reply": "2024-07-14T10:44:46.532926Z"
    },
    "papermill": {
     "duration": 0.090121,
     "end_time": "2024-07-14T10:44:46.535993",
     "exception": false,
     "start_time": "2024-07-14T10:44:46.445872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "game = OomiGame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67a705d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T10:44:46.702193Z",
     "iopub.status.busy": "2024-07-14T10:44:46.701344Z",
     "iopub.status.idle": "2024-07-14T10:45:38.303809Z",
     "shell.execute_reply": "2024-07-14T10:45:38.302836Z"
    },
    "papermill": {
     "duration": 51.769908,
     "end_time": "2024-07-14T10:45:38.388692",
     "exception": false,
     "start_time": "2024-07-14T10:44:46.618784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "win rate - 0.872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.872"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_agent.evaluate(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c94e2e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T10:45:38.553786Z",
     "iopub.status.busy": "2024-07-14T10:45:38.553419Z",
     "iopub.status.idle": "2024-07-14T10:45:38.559395Z",
     "shell.execute_reply": "2024-07-14T10:45:38.558464Z"
    },
    "papermill": {
     "duration": 0.092156,
     "end_time": "2024-07-14T10:45:38.561211",
     "exception": false,
     "start_time": "2024-07-14T10:45:38.469055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q of Hearts'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(game.hands[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0898daa3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T10:45:38.728422Z",
     "iopub.status.busy": "2024-07-14T10:45:38.727521Z",
     "iopub.status.idle": "2024-07-14T10:45:38.734419Z",
     "shell.execute_reply": "2024-07-14T10:45:38.733549Z"
    },
    "papermill": {
     "duration": 0.093691,
     "end_time": "2024-07-14T10:45:38.736313",
     "exception": false,
     "start_time": "2024-07-14T10:45:38.642622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Q of Spades',\n",
       " 'A of Spades',\n",
       " 'K of Diamonds',\n",
       " '10 of Hearts',\n",
       " 'A of Diamonds',\n",
       " 'J of Clubs',\n",
       " 'Q of Diamonds',\n",
       " '8 of Diamonds']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[str(c) for c in game.hands[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3649c05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T10:45:38.913975Z",
     "iopub.status.busy": "2024-07-14T10:45:38.913374Z",
     "iopub.status.idle": "2024-07-14T10:45:38.922905Z",
     "shell.execute_reply": "2024-07-14T10:45:38.921939Z"
    },
    "papermill": {
     "duration": 0.098863,
     "end_time": "2024-07-14T10:45:38.925037",
     "exception": false,
     "start_time": "2024-07-14T10:45:38.826174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "card = best_agent.choose_action({'hand': game.hands[0], 'desk': [game.hands[1][0]], 'played': [],\n",
    "                                                     'current_player': 0, 'tricks_won': {'team1': 0, 'team2': 0},\n",
    "                                                     'trump_suit': \"Diamonds\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d09999a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T10:45:39.087405Z",
     "iopub.status.busy": "2024-07-14T10:45:39.086565Z",
     "iopub.status.idle": "2024-07-14T10:45:39.092336Z",
     "shell.execute_reply": "2024-07-14T10:45:39.091490Z"
    },
    "papermill": {
     "duration": 0.088632,
     "end_time": "2024-07-14T10:45:39.094284",
     "exception": false,
     "start_time": "2024-07-14T10:45:39.005652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10 of Hearts'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "797c9767",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T10:45:39.260628Z",
     "iopub.status.busy": "2024-07-14T10:45:39.260224Z",
     "iopub.status.idle": "2024-07-14T10:45:39.264502Z",
     "shell.execute_reply": "2024-07-14T10:45:39.263597Z"
    },
    "papermill": {
     "duration": 0.089639,
     "end_time": "2024-07-14T10:45:39.266395",
     "exception": false,
     "start_time": "2024-07-14T10:45:39.176756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !rm -rf /kaggle/working/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3dc7e57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T10:45:39.430987Z",
     "iopub.status.busy": "2024-07-14T10:45:39.430606Z",
     "iopub.status.idle": "2024-07-14T10:45:39.434638Z",
     "shell.execute_reply": "2024-07-14T10:45:39.433764Z"
    },
    "papermill": {
     "duration": 0.088213,
     "end_time": "2024-07-14T10:45:39.436521",
     "exception": false,
     "start_time": "2024-07-14T10:45:39.348308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !mkdir /kaggle/working/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ed05e23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T10:45:39.603204Z",
     "iopub.status.busy": "2024-07-14T10:45:39.602825Z",
     "iopub.status.idle": "2024-07-14T10:45:39.607108Z",
     "shell.execute_reply": "2024-07-14T10:45:39.606197Z"
    },
    "papermill": {
     "duration": 0.090287,
     "end_time": "2024-07-14T10:45:39.609084",
     "exception": false,
     "start_time": "2024-07-14T10:45:39.518797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !cp /kaggle/input/rl-model-new/models/backup_model.pth /kaggle/working/models/\n",
    "# !cp /kaggle/input/rl-model-new/models/best_model.pth /kaggle/working/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15216e17",
   "metadata": {
    "papermill": {
     "duration": 0.103594,
     "end_time": "2024-07-14T10:45:39.794988",
     "exception": false,
     "start_time": "2024-07-14T10:45:39.691394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "modelInstanceId": 64932,
     "sourceId": 77235,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21295.231348,
   "end_time": "2024-07-14T10:45:41.250024",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-14T04:50:46.018676",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
